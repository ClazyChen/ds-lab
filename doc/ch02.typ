#import "template.typ": *

= 向量 <vec:chapter>

*线性表*是指相同类型的有限个数据组成的_序列_。
本书按照@邓俊辉2013数据结构 的方法，将线性表分为*向量*（vector）和*列表*（list）两种形式，分别对应C++中的`std::vector`和`std::list`。在另一些教材@严蔚敏1997数据结构 中，这两个词被称为*顺序表*和*链表*，分别对应Java里的`ArrayList`和`LinkedList`。向量（顺序表）和列表（链表）这两对概念通常可以混用。
向量和列表代表着两种最基本的数据结构组织形式：*顺序结构*和*链式结构*。本章介绍顺序结构的线性表，即向量。

== 线性表 <vec:linear-list>

对比线性表和一般的数据结构。一般的数据结构的定义中，数据被组织为“某种结构化的形式”，而在线性表这里它被具体化为了“序列”。既然是序列，那么它会具有头和尾，会具有“第$i$个”这样的概念；每个元素有它在序列上的“上一个”和“下一个”元素。这是一个朴素的想法，从数学角度容易理解。但从计算机的角度，请您思考这样的问题：“元素”应该用什么表示？应该如何找到序列中的一个元素呢？

把计算机中的内存想像为一座巨大的旅馆，线性表是一个居住在其中的旅游团。现在旅游团预定了一大片连续的房间，比如，从1000到1099，并且让第1个游客住在1000，第2个游客住在1001，以此类推，那么我们就很方便地可以知道第$i$个游客的房间号。这种情况下，我们只需要知道游客的序号，就能知道它们居住的房间号。但并不是每个人都会按部就班地居住，一些人可能喜欢阳光，一些人可能想和伙伴们做邻居，于是，这些游客开始交换房间。房间被交换之后，我们再也无法直接知道第$i$个游客的房间号。一个可能的想法是，从1000到1099每一个房间都敲敲门。这种方法虽然可行，但无疑是很低效的。更加糟糕的是，旅游团可能没订到连续的房间，游客们散落在旅馆的各处。因为我们不可能像推销员那样每个房间都敲门（这会被赶出去的），所以再也无法找到我们想要的第$i$个游客了。为了应对这种情况，旅游团的导游往往会记录下各个游客所在的房间号，以便能够找到他们。

在上面这个比喻中，我们可以看到，如果数据结构中的元素被连续地储存，那么我们可以通过它们的序号（称为秩 @邓俊辉2013数据结构，rank）来找到它们；如果数据结构中的元素并没有被连续地储存，则我们只能通过它们的位置（position）来找到它们。上面的三种情况，分别是地址连续、且地址和秩相关的顺序结构（向量）；地址连续、但地址和秩无关的静态链式结构（静态链表）；地址不连续、也和秩无关的动态链式结构（动态链表）。@fig:vec1 示意了三种结构的区别。

#figure(
    image("images/vec1.svg", width: 90%),
    caption: "向量、静态链表和动态链表的对比",
) <fig:vec1>

显然，如果发生了第二种情况，导游通常还是会选择记录房间号而不是逐个敲门。那么既然没有省事，也就没有必要预定一大片房间了。因为旅馆老板（也就是操作系统）可能会乘机宰客。比如，旅游团一次定了100个房间，但中途有50人提前结束了旅行。由于旅游团定的是整单，老板不允许单独退这50个人的房间。于是，旅游团要么承担50间空房的代价（空间损失）；要么再定50个房间，请剩下的50人搬到新房间住（时间损失），然后把原来的100个房间一并退掉。从这个例子中可以看到，静态链表是一个不实用的数据结构，本书将把重点聚焦在向量和动态链表（列表）上。如前所述，向量和列表里定位元素的方法是不同的。向量是循秩访问，而列表则循位置访问。我们通过传入不同的迭代器类型参数来控制这两种访问方式。

```cpp
template <typename T, typename It, typename CIt>
class LinearList : public DataStructure<T> {
public:
    using value_type = T;
    using iterator = It;
    using const_iterator = CIt;

    virtual iterator insert(const_iterator p, const T& e) = 0;
    virtual iterator insert(const_iterator p, T&& e) = 0;
    virtual iterator erase(const_iterator p) = 0;
    virtual const_iterator find(const T& e) const = 0;
    virtual void clear() = 0;
};
```
#graybox[【C++学习】#linebreak()#h(2em)
完整的一个STL容器（哪怕是`std::array`）是非常复杂的，感兴趣的读者可自行阅读其源码。本书的示例代码是一个简化的版本。

*右值引用*（rvalue reference）被用于实现移动语义，即将一个对象的资源（比如内存）袋子_移动_到另一个对象的袋子里，而不是_复制_同样的一份资源。这样可以避免不必要的内存分配和释放，提高程序的性能。容易复制的类型，如基本类型和简单结构体，通常不需要使用右值引用。
]

#graybox[
#h(2em)传统的左值引用通常使用`T&`表示，而右值引用则使用`T&&`表示。以上面示例代码中的两个版本`insert`为例。假设`L`是一个元素类型为`T`的线性表对象：
```cpp
T a {};
L.insert(L.end(), a); // insert(const_iterator, const T&)
L.insert(L.end(), std::move(a)); // insert(const_iterator, T&&)
L.insert(L.end(), T{}); // insert(const_iterator, T&&)
```
#h(2em)当`e`传入一个对象`a`时，`insert`会调用左值引用的版本，因为在`a`被插入的线性表之后，我们希望线性表中的元素`L.back()`和原有的元素`a`是两个不同的元素，也就是说我们希望在`a`所使用的资源之外，再增加一份资源用于`L`中新加入的元素。而当`e`传入_被移动的_对象`std::move(a)`时，后续我们不会再通过`a`访问这个元素，因此我们可以把`a`的资源直接移动到`L`中，而不需要再分配一份新的资源；所以这个时候会调用右值引用的版本。最后，当`e`传入一个_临时对象_`T{}`时，`T{}`是一个临时对象，它的资源不会被其他对象使用，因此我们可以直接移动它的资源到`L`中，也调用右值引用的版本。

*迭代器*（iterator）是STL广泛使用的一种设计模式。迭代器是一个对象，它可以指向容器中的一个元素，也可以通过一些操作来访问容器中的元素，类似于一个带封装的指针。迭代器的设计使得STL的算法可以独立于容器的具体实现。由于迭代器和《数据结构》中的知识点无关，本书中的数据结构会使用一个简化版本的迭代器，以使得一些STL算法以及`range-based for`可以被用于本书中的数据结构。在上面的示例代码中，`const_iterator`是一个只读的迭代器，即被它指向的容器元素不能被修改，而`iterator`是一个可读可写的迭代器。使用迭代器，可以通过如下方式遍历一个`std::vector`对象`V`（以下三种方式是等价的）：
```cpp
for (auto i { 0uz }; i < L.size(); ++i) { visit(V[i]); } // visit by rank
for (auto i { V.begin() }; i != V.end(); ++i) { visit(*i); } // visit by iterator
for (auto& e : V) { visit(e); } // visit by range-based for
```
]

== 向量的结构 <vec:structure>

*向量*（vector）是一个基于*数组*（array）的数据结构。和数组一样，它在内存中占据的是一段连续的空间。#linebreak()
#graybox[#h(2em)
C++建议更多地使用标准库中的向量容器`std::vector`代替数组。向量和数组相比，其最重要的区别在于它是运行时_可变长的_，而在其他使用上，二者基本可以等同。因此，向量上的算法可以很容易地修改为数组上的算法（即使不使用`std::span`）。向量的时间和空间性能和数组相比都只有常数的差异，不会有复杂度的区别，且在大多数场景下二者的性能差异并不显著。因此，除非数组的大小具有确定的语义，否则总是可以使用`std::vector`代替数组。]

作为一种基于数组的线性表，_向量的元素次序和数组的元素次序相同_。如果一个向量`V`基于长度为$n$的数组`A`构建，那么向量`V`的第`i`个元素就是`A[i]`。众所周知，C语言的数组可以视为指针，于是向量`V`的第`i`个元素的地址就是`A+i`。

向量里的元素数量$n$，称为向量的*规模*（size）；而向量所占有的连续空间能够容纳的元素数量$m$，称为向量的*容量*（capacity）。这两者通常是不同的，规模必定不大于容量，而在不超过容量的前提下，向量的规模可以灵活变化，从而赋予了它比数组更高的灵活性。@vec:linear-list 中的旅行团例子也可以帮助理解规模和容量的关系：一个$n$人的旅行团订了连续的$m$个房间，这里$m$可以大于$n$，这样，如果旅行过程中有新人加入团队，他们就可以直接加入到已经预定的连续房间中来。

#figure(
    image("images/vec2.svg", width: 90%),
    caption: "向量、静态链表和动态链表的对比",
) <fig:vec2>

如@fig:vec2 所示，
对于向量中的每一个元素`V[i]`来说，它前面的元素称为它的*前驱*（predecessor），它后面的元素称为它的*后继*（successor）。特别地，和它位置相邻的前驱，也就是*直接前驱*为`V[i-1]`，相应地，*直接后继*为`V[i+1]`。所有的前驱构成了*前缀*（prefix），也就是`V[0:i]`；所有的后继构成了*后缀*（suffix），也就是`V[i+1:n]`。本书中我们用`V[a:b]`来简记`V[a],V[a+1],...,V[b-1]`这个子序列，这是Python的切片（slice）语法，借助它可以简化很多叙述，尤其在记忆一些比较复杂的算法时很有用。

== 循秩访问 <vec:rank-access>

向量的核心特征是*循秩访问*（access by rank）。称元素`V[i]`在向量`V`中的序号`i`为它的*秩*（rank）。对于建立在数组`A`上的向量`V`，因为`V`和`A`的元素次序是一致的，所以`V[i] = A[i] = *(A+i)`。因此，只要知道一个元素的秩，就可以在$O(1)$的时间内访问该元素。因为下标（秩）总是非负的，所以我们用`std::size_t`类型来存储它。

接下来，我们开始构筑向量抽象类。首先，除了在`DataStructure`里定义的规模`size`之外，我们还需要定义容量`capacity`。同时，因为向量是可变长的，所以规模和容量都是可以变化的，还需要两个修改它们的方法。有了修改规模的方法，线性表里的`clear`就可以直接用`resize(0)`实现。其次，我们需要构筑循秩访问的功能，通过重载`operator[]`方法，像访问数组一样访问向量中的元素。

```cpp
template <typename T>
class AbstractVector : public LinearList<T, /* iterators */> {
protected:
    virtual T* data() = 0;
public:
    virtual std::size_t capacity() const = 0;
    virtual void reserve(std::size_t n) = 0;
    virtual std::size_t size() const = 0;
    virtual void resize(std::size_t n) = 0;
    T& operator[](std::size_t r) { // definition (1)
        return data()[r];
    }
    const T& operator[](std::size_t r) const { // definition (2)
        return data()[r];
    }
};
```

#graybox[【C++学习】#linebreak()#h(2em)
在类的成员函数之后加上`const`关键字，表示这个成员函数是一个_只读_的成员函数，它不会修改对象的状态。这样的成员函数可以被`const`对象调用，也可以被非`const`对象调用。但是，`const`对象不能调用非`const`成员函数，因为这些函数可能修改对象的状态。

这会带来一个问题，以`operator[]`为例，当向量`V`是`const`对象时，按照上述定义（1）的`operator[]`无法被调用，也就是说我们无法通过`V[r]`的方式去访问向量中的元素。反之，如果只采用定义（2），那么`const`对象固然可以调用`operator[]`了，但是返回的`V[r]`是一个可修改的元素，这就违背了`const`的语义。

为了解决这个问题，C++引入了`const`成员函数的重载。我们可以引入一个`const`版本的`operator[]`，它返回的是一个只读的元素。这样，`const`对象可以通过`V[r]`的方式去访问向量中的元素，而且返回的元素是只读的。当然这样还有一个编程效率上的小问题，就是我们需要写两个完全相同的函数体。这个问题在C++23引入“`this`捕获器”特性之后得以解决；本书目前使用的编译器暂时不支持该特性。
```cpp
template <typename Self>
auto&& operator[](this Self&& self, std::size_t r) {
    return std::forward<Self>(self).data()[r];
}
```
]

#graybox[
#h(2em)最后，操作底层内存的时候因为不能获得所有权，所以不能使用智能指针，只能使用裸指针。为了避免裸指针被外部获取，我们将向量的获取首地址方法`data()`设置为`protected`，这样只有子类可以访问它。]

现在，我们已经拥有了一个抽象类`AbstractVector`，它被称为*抽象数据类型*（Abstract Data Type，ADT）。ADT可以认为是数据结构的接口，它定义了数据结构的行为，但独立于数据结构的具体实现。如果读者打算自己实现向量类，只需要在抽象类的基础上补充它们即可；读者也可以继承本书提供的示例向量类，重写其中的部分方法。以下将展示本书的示例实现。

```cpp
template <typename T>
class Vector : public AbstractVector<T> {
protected:
    std::unique_ptr<T[]> m_data { nullptr };
    std::size_t m_capacity { 0 };
    std::size_t m_size { 0 };

    T* data() override { return m_data.get(); }
    const T* data() const override { return m_data.get(); }
public:
    std::size_t capacity() const override { return m_capacity; }
    std::size_t size() const override { return m_size; }
};
```

#graybox[【C++学习】#linebreak()#h(2em)
C++14起，允许用户使用智能指针管理数组，因此我们使用`std::unique_ptr`来申请内存。它的主要好处是不需要在析构函数里手动释放，减少了手动管理内存的麻烦。本书中若无特殊情况，将总是使用智能指针来表示所有权，避免在任何地方使用`delete`关键字释放内存。
]

== 向量的容量和规模 <vec:capacity-size>

=== 初始化 <vec:capacity-size:init>

当我们建立一个新的数据结构的时候，有几种情况是比较典型的，应当实现相应的构造函数。在这里，以向量为例展示它们，后面讨论其他的数据结构的时候不再赘述。

*零初始化*。即，生成一个空的数据结构。对于向量来说，这应该包含一个大小为0的数组，并把容量和规模都赋值为0。然而，C++不支持大小为0的数组，所以只能将`data`赋值为`nullptr`，就像在@vec:rank-access 中展示的默认值那样。

*指定大小的初始化*。即，给定$n$，生成一个规模为$n$的数据结构，其中的每个元素都采用默认值（即元素采用零初始化）。对于向量而言，可以申请一片大小为$n$的内存，如下面的代码所示。
```cpp
Vector(std::size_t n) : m_data { std::make_unique<T[]>(n) }
                      , m_capacity { n }
                      , m_size { n } {}
```

#h(2em)*复制初始化*。即，给定相同数据结构的一个对象，复制该对象里的所有数据及数据之间的结构化关系。对于向量来说，只需要在申请大小为$n$的内存之后，将给定的向量的元素逐一复制进来即可。注意这里在初始化器中显式调用了上面的“指定大小的初始化”的构造函数，为向量进行了初步的初始化，然后再把另一个向量的数据复制进来。
```cpp
Vector(const Vector& other) : Vector(other.m_size) { 
    std::copy_n(other.m_data.get(), other.m_size, m_data.get());
}
```

#h(2em)*移动初始化*。即，给定相同数据结构的一个对象，将该对象里的所有数据及数据之间的结构化关系移动到当前对象处。如前所述，*移动*（move）语义和复制（copy）有显著的不同，因为在移动之后，“被移动”的对象失去了对数据的所有权，我们永远不会从被移动后的对象里访问那些数据。
```cpp
Vector(Vector&& other) noexcept : m_data { std::move(other.m_data) }
                                , m_capacity { other.m_capacity }
                                , m_size { other.m_size } {
        other.m_capacity = 0;
        other.m_size = 0;
    }
```

#graybox[【C++学习】#linebreak()#h(2em)
移动构造函数和移动赋值运算符通常被声明为`noexcept`，因为如果一个对象在移动过程中抛出异常，那么该对象可能处于一个无效的状态。而由于移动语义的设计，源对象在移动操作后可能不再保留其原有状态，这使得异常处理变得困难。此外，应当避免抛出异常的析构函数也通常被声明为`noexcept`。

通过对智能指针调用`std::move`，我们可以在常数时间内，将被移动对象的数据转移到新对象里。被移动之后，`other`的数据区被复位为空指针，规模和容量都被置0，就像它被零初始化了一样，成为了一个空向量。

从@fig:vec3 中可以直观了解到复制和移动的语义区别。当数据结构里的元素数量很多时，逐元素地复制是一项复杂、琐碎、漫长的工程，而移动则是一项简单、整体、快速的工作。在本书的示例代码中，我们经常会给同一个函数提供一个复制版本和一个移动版本。
]

#figure(
    image("images/vec3.svg", width: 100%),
    caption: "复制和移动的语义区别",
) <fig:vec3>

#graybox[
#h(2em)比如，对于插入（`insert`），我们定义一个插入`const T&`类型的方法用于复制（不破坏源），又定义了一个插入`T&&`类型的方法用于移动（破坏源）。这些方法之间往往只有一个或几个`std::move`的区别，因此本书通常省略移动版本的方法，而只展示复制版本的方法。尽管如此，在读者的日常编程中需要注意，只要复制和移动的时间成本_有可能_相差比较远，就应该同时定义并实现复制和移动两个版本的方法，而不应该只实现复制。

需要注意的是，析构函数、复制构造函数、移动构造函数、复制赋值运算符、移动赋值运算符这5个函数，一旦显式定义其中的一个（比如想要定义复制构造函数），编译器就不会自动生成其他的函数。处于“一荣俱荣，一损俱损”的关系。因此，我们在日常编程的时候，通常选择不实现它们中间的任意一个函数（0原则，rule of zero）。因为日常编程的时候，通常都使用的是STL对象，而STL里已经将这些功能实现了。但是，在我们实现一些比较底层的结构时候，没法依靠STL里的实现，需要实现这5个函数中的一个或几个。此时，就必须要将所有的5个函数实现（5原则，rule of five）。我们可以使用`=default`来显式使用自动生成的函数，但如果我们不显式说明它，这些函数将不会被包含在这个类中。
]

*初始化列表初始化*。即，使用初始化列表`std::initializer_list`对数据结构进行初始化。初始化列表也是一个典型的容器，可以直接使用STL方法移动。

```cpp
Vector(std::initializer_list<T> ilist) : Vector(ilist.size()) {
    std::move(ilist.begin(), ilist.end(), m_data.get());
}
```

#graybox[
#h(2em)支持初始化列表初始化之后，我们就可以用下面的形式来初始化一个向量（正如`std::vector`一样）。
```
Vector V { 1, 2, 3 };
```
]

=== 装填因子 <vec:capacity-size:load-factor>

设向量的容量为$m$，规模为$n$，则称比值$n / m$为*装填因子*（load factor）。正常情况下，这是一个$[0,1]$之间的数。装填因子是衡量向量效率的重要指标。

+ 如果装填因子_过小_，则会造成内存浪费：申请了巨大的数组，但其中只有少量的单元被向量中的元素用到，其他单元都被闲置了。
+ 如果装填因子_过大_（超过1），则会引发数组越界，造成_段错误_（segmentation fault）。

#h(2em)
刚开始时，装填因子一定在$[0,1]$之间。
但因为数组的容量$m$是固定的，而向量的规模$n$是动态的，所以一开始分配的$m$可能后来会不够用，从而产生装填因子大于1的问题，此时就需要令$m$增大，这一操作称为*扩容*（expand）。相反，如果向量的规模$n$变得很小，那么$m$可能会远大于$n$，这时就需要令$m$减小，这一操作称为*缩容*（shrink）。扩容和缩容的操作都是需要花费时间的，因此我们希望尽量减少它们的发生次数。在对时间性能要求非常严格的场景下（如算法竞赛），有可能禁止扩容和缩容，转为预先分配足够大的内存，这样可以避免动态分配内存的时间开销。

在一般场景下，扩容是非常常见的操作，但现实中很少进行缩容。扩容和缩容都需要时间，在扩容的场合是实现可变长特性的必需，但在缩容的场合仅仅是节约了空间而已。有多个原因让我们不愿意实现缩容：
    + 我们可以接受一定程度的空间浪费，因为很少有程序能占满全部的内存。
    + 如果缩容之后，又因为规模扩大而不得不扩容，一来一回浪费了不少时间，而价值甚微。
    + 当不得不考虑空间时，我们有_手动缩容_的备用方案。即，复制初始化生成一个新向量，然后清空原向量来释放内存；按照之前介绍的方法，这个新向量的装填因子为1，处在空间最大利用的状态。

#h(2em)因此，本书的示例代码中没有实现缩容。

=== 扩容 <vec:capacity-size:expand>

无论是扩容还是缩容，我们都需要重新申请一片内存。在扩容的场合，这很好理解。我们预定了1000到1099的房间，但在我们预定之后，1100号房间可能被其他旅客占用了。这时如果我们想要预定连续的200个房间，就需要重新找一段空房间。缩容的场合，则是为了安全性考虑，不允许释放数组的部分内存。重新申请内存之后，我们将数据复制到新内存中。下面是扩容的一个实现。

```cpp
void reserve(std::size_t n) override {
    if (n > m_capacity) {
        auto tmp { std::make_unique<T[]>(n) };
        std::move(m_data.get(), m_data.get() + m_size, tmp.get());
        m_data = std::move(tmp);
        m_capacity = n;
    }
}
```

#figure(
    image("images/vec4.svg", width: 100%),
    caption: "扩容的过程",
) <fig:vec4>


@fig:vec4 展示了当插入元素时如果发现容量不足，所进行的扩容过程。其中释放原先的内存这一步，本书中所采用的智能指针会自动完成，而C语言和旧标准C++则需要显式地`delete[]`释放内存。

可以看出，扩容是一项成本很高的操作，因为它需要开辟一块新的内存。设扩容之后的容量为$m$，则扩容算法的时间复杂度为$Theta(m)$。由于$m$可能会很大，我们不希望经常扩容。在@vec:capacity-size:arithmetic-geometric 里将会讨论一些扩容策略。

```cpp
void resize(std::size_t n) override {
    if (n > m_capacity) {
        reserve(n);
    }
    m_size = n;
}
```

#h(2em)如上所述，我们还设计了一个方法`resize`用来改变规模，当规模超过容量（装填因子超过1）的时候调用`reserve`扩容。需要注意的是，一些其他的方法也会改变规模，比如插入方法`insert`会让规模增加1，而删除方法`remove`会让规模减1。我们需要在实现插入方法的时候也考虑扩容问题；_如果实现了缩容，那么在实现删除方法的时候也需要考虑缩容问题_。

=== 扩容策略 <vec:capacity-size:expand-strategy>

当我们调用`resize`的时候，可以立刻知道，需要扩大到多少容量才能容纳目标的规模。但实际情况下，很多时候元素是被一个一个加入到向量中的，这个时候，按照`resize`的策略，每次都扩容到新的规模，是一个糟糕的选择。假设初始化为了一个规模为$n$的向量，然后元素一个一个被加入，那么按照$n arrow.r n+1 arrow.r n+2 arrow.r ...$的次序扩容，每加入一个元素，都会造成至少$n$个元素的复制，时间效率极差。

因此，在面对持续插入的时候，我们需要设计新的扩容策略，以降低扩容发生的频率。这个策略应该是由向量的设计者提供的，而不是用户：如果用户知道更加合适的策略，他们会主动使用`reserve`进行手动扩容。但是，用户通常没有精力用在这种细节上；这个时候，向量的设计者提供的自动扩容策略就会成为一个不错的备选项。

现在我们尝试为扩容策略的问题添加一个抽象的描述。当我们讨论扩容的时候，显然不需要知道向量中的数据内容是什么。因此，扩容策略作为一个算法，输入向量的当前规模$n$和当前容量$m$，输出新的容量$m'$。_这个描述具有良好的可重用性，它同样可以用于缩容。_

```cpp
class VectorAllocator : 
    public Algorithm<std::size_t(std::size_t, std::size_t)> {
protected:
    virtual std::size_t expand(std::size_t capacity
                             , std::size_t size) const = 0;
    virtual std::size_t shrink(std::size_t capacity
                             , std::size_t size) const {
        return capacity;
    }
public:
    std::size_t operator()(std::size_t capacity
                         , std::size_t size) override {
        if (capacity <= size) {
            return expand(capacity, size);
        } else {
            return shrink(capacity, size);
        }
    }
};
```

#h(2em)
基于上述的分析，我们可以用这个类表示容量改变的策略。在@vec:capacity-size:arithmetic-geometric，将继承这个类并重写`expand`方法，以实现不同策略的扩容。缩容方法则直接返回`capacity`（永不缩容）；如果需要实现缩容的时候，也可以使用这个模板，重写`shrink`方法。

#pagebreak()#h(2em)
=== 等差扩容和等比扩容 <vec:capacity-size:arithmetic-geometric>

那么，应该如何设计扩容策略呢？一个简单的想法是，既然每次容量+1不行，那就加多一点。这种思路可以被概括为_等差数列_扩容方法。如果选取$d$作为公差，那么在本节开始的那个例子中，将按照$n arrow.r n+d arrow.r n+2d arrow.r ...$的次序扩容。

```cpp
template <std::size_t D> requires (D > 0)
class VectorAllocatorAP : public VectorAllocator {
protected:
    std::size_t expand(std::size_t capacity
                     , std::size_t size) const override {
        return capacity + D;
    }
};
```

#graybox[【C++学习】#linebreak()#h(2em)
`requires`表示模板参数必须要满足后面的条件，否则无法通过编译。当`D`为0时，等差数列扩容的公差是0，这是一个无意义的操作，因此我们禁止这种情况的发生。使用`requires`代替传统的`std::enable_if`有助于简化在模板元编程中限制，有助于更好地实现SFINAE（Substitution Failure Is Not An Error）原则。

SFINAE原则是模板元编程中的重要原则。当使用了SFINAE实现对于同一个函数模板的多个重载时，对于函数模板的每一个实例，编译器会尝试将实参代入到模板参数中，如果代入失败，编译器会尝试下一个重载，而不是报错。比如，我们可以同时定义带有`requires std::is_same_v<T, int>`以及`requires std::is_same_v<T, std::string>`的两个重载，当传入一个`int`时，编译器会选择第一个重载，而传入一个`std::string`时，编译器会选择第二个重载。
]

既然有了等差数列，另一个很容易想到的方法是按照_等比数列_扩容。如果选取$q$作为公比，则会按照$n arrow.r q n arrow.r q^2n arrow.r ...$的次序扩容。

```cpp
template <typename Q> requires (Q::num > Q::den)
class VectorAllocatorGP : public VectorAllocator {
protected:
    std::size_t expand(std::size_t capacity
                     , std::size_t size) const override {
        std::size_t newCapacity { capacity * Q::num / Q::den };
        return std::max(newCapacity, capacity + 1);
    }
};
```

#graybox[
#h(2em)这里允许了C++11提供的编译期有理数`std::ratio`作为模板参数，`Q::num`表示分子，而`Q::den`表示分母。
]

请注意，需要保证新的容量比原有容量大，否则扩容就没有意义。上面的做法保证了容量至少会扩大1。比如，当$Q=3/2$，往容量为0的向量里连续插入元素时，容量变化为$0 arrow.r 1 arrow.r 2 arrow.r 3 arrow.r 4 arrow.r 6 arrow.r 9 arrow.r ...$，如果没有容量至少扩大1的设计，等比扩容将永远停留在0容量。

=== 分摊复杂度分析 <vec:capacity-size:amortized-analysis>

很显然，进行单次扩容操作的时候，等差扩容的时间复杂度为$O(n+d) = O(n)$，等比扩容的时间复杂度为$O(q n) = O(n)$（因为$q$是常数），两者看起来没有区别；甚至和我们已经知道效率很低的情况（$d=1$的等差扩容）也没有区别。这也意味着，我们评价时间效率的方法可能出现了一些问题。

问题的关键在于，我们设计扩容策略的目的是按照等差或等比的_数列_扩容，而不是_一次_扩容。所以，评价这两种扩容规则的标准，不是进行一次扩容的效率或进行一次扩容后的装填因子，而是比较一系列扩容操作的总体效率和在这一系列扩容操作中的平均装填因子。用已有的复杂度分析工具不足以对这两种策略的效率进行准确评价。
为了对_一系列_操作进行分析，需要引入新的复杂度分析标准。

一般地，假设$O_1, O_2, ..., O_n$是连续进行的$n$次操作，则当$n arrow.r infinity$，这$n$次连续操作所用时间的_平均值_的复杂度，称为这一操作的*分摊复杂度*（amortized complexity），对分摊复杂度的分析称为*分摊分析*。分摊分析的原则之一是：_使用相同效果的操作序列_。所以，要比较上述两种算法，不应该把每次操作取为“进行一次扩容”（因为两种方法扩容量不一样），而应该取为“向量`V`的规模增加$1$”。连续进行$n$次操作，就可以考虑向量`V`的规模从$0$增长为$n$的过程。

在等差扩容方法中，容量依次被扩充为$d,2d,3d,...,n$，共进行$n/d$次扩容。
因此，分摊复杂度为：
$
T(n) = (d + 2d + 3d + ... + n) / n 
     = 1 / 2 dot (d / n + 1) dot (n / d) 
     = Theta(n / d)
$

#h(2em)
另一方面，从空间效率的角度，进行$k$次扩容之后的装填因子至少为$(k d ) / ((k+1) d)=k / (k+1)$，当$k arrow.r infinity$时，装填因子趋于100%。

而在等比扩容方法中，容量被依次扩充为$q,q^2,q^3, ...,n$，共进行$log _q n$次扩容。因此，分摊复杂度为：
$
T(n) = (q + q^2 + q^3 + ... + n) / n 
     = q / (q-1)
     = O(1)
$

#h(2em)
另一方面，装填因子不断在$[1 / q,1]$之间线性增长，平均装填因子为$(1+q) / (2q)$。可以看出，不管怎样选择$q$，对分摊复杂度都没有影响，而更小的$q$能够带来更大的平均装填因子。当选择$q=2$时，平均装填因子为75%。

可以看出，等比扩容的装填因子并没有很低，而换来了分摊时间复杂度上巨大的优化。因此，我们倾向于选择等比扩容。至于等比扩容的公比，则是一个值得讨论的话题。从上面的推导中，我们发现分摊时间复杂度的系数为$q / (q-1)$，它会随$q$的增加而降低；另一方面，平均装填因子也会随$q$的增加而降低。因此，选择更大的$q$，事实上是以时间换空间的做法。

因为分摊$O(1)$已经很快，所以通常选取的$q$比较小。常见的公比选择有2和$3/2$。GCC、Clang和@邓俊辉2013数据结构 选择的公比是2；而MSVC则采用更节约空间的$3/2$。

需要指出的是，等比扩容也存在一些劣势：容量越大，装填因子不高带来的空间浪费愈发明显，所以有些对空间要求较高的情况下，也采用二者相结合的方式：在容量比较小时等比扩容、在容量比较大的时候等差扩容。这种思想在《网络原理》里的_慢启动_中得到了应用。

// TODO: 习题：等比缩容问题
// TODO: 习题：分摊复杂度分析的例子

== 插入、查找和删除 <vec:insert-remove-find>

对于任何数据结构，都有三种基本的操作：
+ *插入*（insert）：向数据结构中插入一个元素。
+ *查找*（find）：查找一个元素在数据结构中的位置。
+ *删除*（delete）：从数据结构中移除一个元素。

#h(2em)
在这一节中，我们以向量为例介绍这三种基本的操作。

=== 插入一个元素 <vec:insert>

要将待插入的元素`e`插入到`V[r]`，那么可以将原来的向量`V[0:n]`分成`V[0:r]`和`V[r:n]`两部分。
- 插入之前，向量是`V[0:r]`，`V[r:n]`。
- 插入之后，向量是`V[0:r]`，`e`，`V[r:n]`。
#h(2em)在插入的前后，前一段`V[0:r]`的位置是不变的，而后一段`V[r:n]`需要整体向后移动1个单元的位置。据此，可以设计@fig:vec5 所示的算法，其时间复杂度为$Theta(n-r)$，和插入位置相关，这类算法被称为*输入敏感*（input-sensitive）的。

#figure(
    image("images/vec5.svg", width: 40%),
    caption: "往向量里插入一个元素",
) <fig:vec5>

```cpp
iterator insert(iterator p, const T& e) override {
    if (m_size == m_capacity) {
        reserve(m_allocator(m_capacity, m_size));
    }
    std::move_backward(p, end(), end() + 1);
    *p = e;
    ++m_size;
    return p;
}
```

#graybox[【C++学习】#linebreak()#h(2em)
为了让我们在 @vec:capacity-size:expand-strategy 定义的扩容策略被嵌入到向量里来，我们可以为向量模板增加一个参数，写成下面这样的形式。
```cpp
template <typename T, typename A = VectorAllocatorGP<std::ratio<3, 2>>
    requires std::is_base_of_v<VectorAllocator, A>
class Vector : public AbstractVector<T> { /* ... */ };
```
#h(2em)
在上面这个模板参数表声明中，我们规定模板的第二个参数`A`必须是我们上面实现的`VectorAllocator`的派生类。用户可以不显式地指定扩容策略，而选择我们默认的策略（公比为$3 / 2$的等比扩容）；也可以使用自定义的扩容策略。加入这个参数之后，我们只需要在`insert`的开头进行一次判断，就可以自动地在插入满向量时进行扩容。
]

=== 平均复杂度分析 <vec:average-analysis>
为了更定量地分析插入操作的时间效率，引入一个新的复杂度分析策略：*平均复杂度*。

在介绍复杂度时曾经强调，复杂度是依赖于数据_规模_，不依赖于输入_情况_的分析手段。在上面的插入算法中，数据规模通常认为是$n$，而$r$是具体情况带来的参数。为了研究不同具体情况对算法时间效率的影响，有三种常见的分析手段：
+ *最坏时间复杂度*：研究在情况最坏的情况下的复杂度。很多算法有硬性的时间限制（如在算法比赛中，通常要求输出结果的时间不能多于1s或2s），此时常常使用最坏时间复杂度分析。这是最常用的时间复杂度分析。
+ *最好时间复杂度*：研究在情况最好的情况下的复杂度。研究最好时间复杂度的意义远小于最坏时间复杂度。最好时间复杂度有时用于嘲讽某种算法的效率：在最好的情况下，这种算法的复杂度也只能达到（某个复杂度），而我的新算法在最坏的情况下也可以达到（某个更好的复杂度）。
+ *平均时间复杂度*：研究在平均情况下的复杂度。如果没有硬性的时间限制，则平均时间复杂度往往能更好地反映一个算法的总体时间效率。平均时间复杂度需要知道每种情况发生的_先验_概率，在这个概率的基础上计算$T(n)$的_数学期望_的复杂度。在针对现实数据的实验研究中，常见的假设包括正态分布、帕累托（Pareto）分布和泊松（Poisson）分布；而在《数据结构》学科中，通常假设成_等可能的分布_，以方便进行理论计算。就分摊时间复杂度一样，平均时间复杂度的计算也经常是非常复杂且困难的，只需要了解其基本的技术即可。

#h(2em)平均复杂度很容易和分摊复杂度发生混淆，需要加以区分。下面是它们的一些典型的差异：
+ 分摊复杂度是_一系列_连续操作的_平均_效率，而平均复杂度是_单次_操作的_期望_效率。
+ 分摊复杂度的一系列连续操作是有可能（通常都）存在后效的，而平均复杂度只讨论单次操作的可能情况。
+ 分摊复杂度需要指定每次进行何种的_基本操作_，而平均复杂度需要指定各种情况的_先验概率_。
+ 分摊时间复杂度依赖于一系列连续操作，但不同的操作序列可能导致不同的分摊复杂度。因此，分摊时间复杂度可以叠加“最坏”、“最好”和“平均”的修饰词，而平均时间复杂度是和最坏、最好并列且互斥的。

#h(2em)
最坏、最好、平均时间复杂度对应统计里的_最大值_、_最小值_和_数学期望_。显然，其他统计量，比如_方差_，在分析的时候也是有价值的，也深得科研人员重视，但它超出了本书和一般计算机工程师需要掌握的范围。

现在回到插入的算法，它的时间复杂度是$Theta(n-r)$（不考虑扩容）。显然，最好时间复杂度是$O(1)$（插入在末尾的情况），最坏时间复杂度是$Theta(n)$（插入在开头的情况）。这里有略微不严谨的地方，因为`r`的最大值可以取到`n`，此时`n-r=0`，不再符合复杂度记号的定义；不过，因为我们清楚任何算法的时间都不可能为0，所以一般不在这个细节上做区分。

为了求平均时间复杂度，一个合理的假设是，`r`的取值对于`[0:n+1]`之间的整数是等概率的（注意有`n`个可以插入的位置，而不是`n-1`个）。在这个假设下，容易算出单次插入的平均时间复杂度为$Theta(n)$。

=== 查找一个元素 <vec:find>
查找需要返回被查找元素所在的位置。和插入、删除相比，查找具有更加丰富的灵活性，甚至于一些编程语言（如SQL）的核心就是查找。

最简单的查找是_按值查找_。即，给定被查找元素的值，在数据结构中找到等于这个值的元素所在的位置。对于更加复杂的查找类型，比如_按区间查找_（给定被查找元素所在区间）等，人们设计了更加复杂的数据结构来应对。对于按值查找的问题，最简单的方案就是检测向量中的每个元素是否等于要查找的元素`e`，如果等于，就把它的秩返回。

```cpp
iterator find(const T& e) const override {
    for (auto it { begin() }; it != end(); ++it) {
        if (*it == e) {
            return it;
        }
    }
    return end();
}
```

#graybox[【C++学习】#linebreak()#h(2em)
关于找不到的情况，有多种处理方式。上面采用的方法是返回无效迭代器，除了返回序列尾部溢出的`end()`之外，返回序列头部溢出的`--begin()`也是常见的选择。此外，也可以将返回值的类型改为`std::optional<iterator>`，当找不到的时候返回一个无效值。
]

设`e`在向量中的秩为`r`，那么在_查找成功_的情况下，上述算法的时间复杂度为$Theta(r)$。在_查找失败_的情况下，算法的时间复杂度为$Theta(n)$。这里可以分析，在等可能条件下，查找成功时的平均时间复杂度是$Theta(n)$。
注意，查找成功的概率是一个很难假设的值，所以在分析平均时间复杂度时，通常只分析“查找成功时”和“查找失败时”的平均时间复杂度，而不会将它们混为一谈。

因为对于向量`V`和待查找元素`e`的情况没有更多的先验信息，所以暂时也没有比上面更高效的解决方案。*利用信息思考*是计算机领域重要的思维方式。在设计算法时，应尽可能利用更多的先验信息。反之，如果先验信息不足，则算法的效率受到信息论限制，不可能会特别高。这个思维方式在后文介绍各种算法的设计过程时，还会反复出现。

=== 删除一个元素 <vec:remove>
删除元素是插入元素的逆操作。在插入元素时，让被插入元素的后继_后移_；因此在删除元素的时候，只需要让被删除元素的后继_前移_即可。需要注意前移和后移在方向上的差别，插入时的`std::move_backward`，逆操作应该是`std::move`。

```cpp
iterator erase(iterator p) override {
    std::move(p + 1, end(), p);
    --m_size;
    return p;
}
```

#h(2em)
和插入一样，可以分析出删除操作时间复杂度$Theta(n-r)$，平均时间复杂度$Theta(n)$，空间复杂度$O(1)$。到此为止，我们实现了完整的`Vector`类，可以开始实验了。如果您自己完成了向量的设计，可以使用#bluetxt[`vector.cpp`]进行简单的功能测试，并在实验中将`dslab::Vector`替换为自己的向量类。

=== 插入连续的元素 <vec:insert-continuous>

#bluetxt[实验`vins.cpp`。]在这个实验中，我们将用实验观察等差扩容和等比扩容的时间效率差异。因为评估的是分摊时间，所以需要构造一个插入连续元素的场景来进行观察。从@vec:insert 中我们知道，在位置`r`插入一个元素的时间复杂度为$Theta(n-r)$。为了降低插入连续元素这个操作本身对实验结果的影响，更好地观察扩容时间，我们固定每次都在向量的末尾插入元素。

#figure(table(
    columns: 6,
    $n$, $d=64$, $d=4096$, [$q$ = $3/2$], [$q$ = $2$], [$q$ = $4$],
    [200K], [444], [8], [1], [1], [0],
    [400K], [2241], [36], [1], [1], [1], 
    [600K], [3649], [59], [1], [2], [0],
    [800K], [5474], [74], [3], [0], [1], 
    [1M], [6388], [96], [2], [2], [2], 
),
    caption: "数组求和算法的时间",
) <tab:vec1>

如@tab:vec1 所示，在实验的示例代码中，我们比较了$d=64$、$d=4096$、$q=3/2$、$q=2$、$q=4$这些情况。可以发现，当$n$比较小的时候，差异不明显；而当$n$比较大，如达到$10^6$的量级时，$d=64$会极其缓慢，而$d=4096$也慢慢和三种等比扩容拉开距离（如果您增加一个$n=10^7$的用例，会更加明显）。相反，三种等比扩容的区别非常微小，因此我们可以判断出，这已经非常接近插入这些元素本身需要的时间，和扩容的关系不大。

=== 向量合并 <vec:concat>

#bluetxt[实验`vcat.cpp`。]在@vec:insert-continuous 中讨论的是连续插入元素情况，实际上不应该存在。一方面，这种情况不应该使用向量的自动扩容策略，而应该预先调用`reserve`函数进行预分配。这样可以避免不必要的扩容操作，提高效率。另一方面，因为要插入的不是单个元素，而是多个元素，插入操作的处理方式也可以有所不同。本节讨论一次性插入多个元素的问题，亦即向量合并问题。假设我们有两个向量`V[0:n]`和`W[0:m]`，我们希望将整个`W`插入到`V`的位置`r`处，实现向量合并。

如果我们采用@vec:insert 中介绍的方法，将`W`中的元素一个一个插入到该位置，那么时间复杂度会高达$Theta((n-r) dot m)$，平均$Theta(n dot m)$，略显笨重。

```cpp
Vector<T>& operator()(std::size_t r) override {
    auto pos { V.begin() + r };
    for (auto&& e : W) {
        V.insert(pos++, std::move(e));
    }
    return V;
}
```

#h(2em)读者可以敏锐地发现，只要再次使用在讨论单元素插入时的分析方法，就可以得到更加高效的算法。要将待插入的向量`W`插入到`V[r]`，那么可以将原来的向量`V[0:n]`分成`V[0:r]`和`V[r:n]`两部分。
+ 插入之前，向量是`V[0:r]`，`V[r:n]`。
+ 插入之后，向量是`V[0:r]`，`W`，`V[r:n]`。其中，`V[r:n]`被转移到了`V[r+m:n+m]`的位置上。

#h(2em)这样设计出了一个批量插入的算法，两种方法的对比如@fig:vec6 所示。

```cpp
Vector<T>& operator()(std::size_t r) override {
    V.resize(V.size() + W.size());
    std::move_backward(V.begin() + r, V.end() - W.size(), V.end());
    std::move(W.begin(), W.end(), V.begin() + r);
    return V;
}
```

#figure(
    image("images/vec6.svg", width: 100%),
    caption: "向量合并的两种方法",
) <fig:vec6>

在示例程序中已经预先`reserve`，所以这里`resize`可以保证在$O(1)$时间里完成。
示例程序中进行了四类情况的评估：插入少量元素到开头、插入大量元素到开头、插入少量元素到结尾、插入大量元素到结尾。这里的少量可以认为是常数$m = O(1)$，而大量可以认为$m = Theta(n)$。

#figure(table(
    columns: 3,
    [实验场景], [逐个插入], [批量插入],
    [插入少量元素到开头], [2], [0],
    [插入大量元素到开头], [1257], [0], 
    [插入少量元素到结尾], [0], [0],
    [插入大量元素到结尾], [0], [0],
),
    caption: "向量合并算法的时间",
) <tab:vec2>

在@tab:vec2 中，$n=2 times 10^5$，少量元素$=10^2$，大量元素$=10^5$。可以看出，当插入元素到结尾时，两种方法的时间性能接近，而插入元素到开头时，二者有着巨大的差异，尤其是在插入大量元素时。批量插入的算法中体现出的_用块操作代替多次单元操作_的思想，在以线性表为背景的算法设计问题中被广泛使用。

=== 按值删除元素 <vec:remove-value>

#bluetxt[实验`vrm.cpp`。]在 @vec:remove ，我们讨论的删除是按位置删除，具体到向量就是按_秩_删除。另一种删除的方式是按_值_删除，也就是说，我们给定一个元素`e`，想要删除向量中和`e`相等的每一个元素。

在分析这个问题之前，先对_按值_操作进行一些深层次的理解。我们知道，数据结构是元素的集合，元素之间是互不_相同_的，但这并不妨碍它们_相等_，因为我们可以定义“相等”（反映到C++中，就是重载运算符`operator==`）。比如说，我们定义值相同为“相等”，这样就不需要考虑元素的地址；这是按值删除的思路。顺着这个思路，我们可以扩展按值删除到更加一般的_按条件_删除。比如说，向量里的元素是一个结构体，我们定义结构体的某个属性相同为“相等”，而其他属性可以不被考虑；此时，某个属性等于给定的值就是我们定义的_条件_。
#graybox[【C++学习】#linebreak()#h(2em)
在C++的STL中，按值删除对应的算法是`std::remove`，而按条件删除对应的算法是`std::remove_if`，二者具有高度的相似性。其他的一些算法也有相应的“按条件”版本，读者可以根据需要自行了解，这里不再赘述。]

下面我们来讨论按值删除元素的问题。为了评估算法的性能，示例程序构造了一个场景：在一个规模为`n`的向量中，第奇数个元素为1，第偶数个元素为0，我们的算法将要删除所有的0，也就是说删除一半的元素。和 @vec:concat 一样，我们从最朴素的想法开始：逐个查找、逐个删除。我们在向量`V`中查找要删除的`e`，每发现一个就删除一个，直到没有等于`e`的元素为止。如@fig:vec7 所示。

```cpp
void remove(const T& e) override {
    while (true) {
        if (auto r { std::find(V.begin(), V.end(), e) }; r != V.end()) {
            V.remove(r);
        } else {
            break;
        }
    }
}
```

#h(2em)
上述朴素算法的空间复杂度是$O(1)$，但是时间效率是很低的。这里使用的`std::find`，原理和@vec:find 介绍的查找一样，时间复杂度为$Theta(r)$。而另一方面，按照@vec:remove 的介绍，在找到之后`erase`的时间复杂度为$Theta(n-r)$。所以，每次循环的时间复杂度为$Theta(n)$。在极端情况下（比如我们设计的实验场景），有$Theta(n)$个元素要被删除，此时时间复杂度为$Theta(n^2)$。


#figure(
    image("images/vec7.svg", width: 60%),
    caption: "向量按值删除元素：逐个查找、逐个删除",
) <fig:vec7>

在设计算法的时候，不一定能直接想到最佳的复杂度。这个时候，可以对比一下_相似问题_的复杂度。比如，之前介绍讨论批量插入（向量合并）的时候可以做到线性的时间复杂度，没有理由批量删除（按值删除）需要平方级的时间复杂度。因此，我们需要寻找提高时间效率的切入口。

为了降低时间复杂度，就要设法降低在算法中进行的_不必要工作_。在不必要工作中，有几种比较典型。一种是_不到位工作_，它代表了在算法中，本能够一步到位的计算被拆分成了若干个碎片化的步骤，而产生的时间浪费。比如在向量合并的逐个插入算法中，后缀`V[r+1:n]`就接连移动了`m`次；我们通过合并这些移动实现了算法优化。

另一种是_重复工作_，它代表了在算法中，重复计算了同一算式造成的时间效率浪费。在上面的朴素的按值删除算法中，就有一项非常明显的重复工作。如@fig:vec7 所示，第一次检索了第一段`V1`，第二次检索了`V1+V2`，第三次检索了`V1+V2+V3`；这里`V1`被检索了3次，`V2`被检索了2次，而事实上只需要检索一次即可。在我们查找完毕的时候，可以记录下当前查找到的位置；下一次查找的时候从记录下的位置开始记录，如@fig:vec8 所示。

```cpp
void remove(const T& e) override {
    auto r { V.begin() };
    while (r = std::find(r, V.end(), e), r != V.end()) {
        V.erase(r);
    }
}
```

#figure(
    image("images/vec8.svg", width: 60%),
    caption: "向量按值删除元素：全局查找、逐个删除",
) <fig:vec8>

我们发现上述算法的优化仍然无法提高最坏时间复杂度。在最坏的情况下（所有元素都要被删除），光是`erase`就要花费$Theta(n^2)$的时间，所以上面这个算法的优化程度仍然不够。因此，下一步优化就要从`erase`入手，需要将`erase`的工作展开来，看看其中哪些是不必要的。

在`erase`中，主要消耗时间的是_元素移动_的操作。可以发现，如果`V[0:i]`中有`k`个元素要删除，那么最后一个元素`V[i-1]`就要向前移动`k`次：依次移动到`V[i-2]` $arrow.r$ `V[i-3]` $arrow.r ... arrow.r$ `V[i-k-1]`的位置上。比如，在 @fig:vec8 中，`V3` 就移动了2次。
您可以敏锐地发现这正是前面所讲述的_不到位工作_。这一系列的移动被拆成了`k`次，而实际上是可以一步到位，直接从`V[i-1]`移动到目标位置`V[i-k-1]`的。

为什么可以直接移动到目标位置呢？注意到，无论是上面的哪一种算法，当检索到`V[i]`的时候，前缀`V[0:i]`的所有元素都已经被检索过了，因此`k`的值已经确定了，并且前`i-1`个元素已经移动到了正确的目标位置。因此，`V[i]`的移动是可以直接到位的。这样，我们就可以设计出一个更加高效的算法。

```cpp
void remove(const T& e) override {
    auto fp { V.begin() }, sp { V.end() };
    while (fp != V.end()) {
        if (*fp != e) {
            *sp++ = std::move(*fp);
        }
        ++fp;
    }
    V.resize(sp - V.begin());
}
```

#graybox[【C++学习】#linebreak()#h(2em)
利用STL，上述算法有一个简单的写法：
```cpp
void remove(const T& e) override {
    V.resize(std::remove(V.begin(), V.end(), e) - V.begin());
}
```
#h(2em)这里使用了STL提供的`std::remove`，它并不会真正地将`e`删除，而是将非`e`的元素都移动到向量的前半部分，并返回新的尾部迭代器。用户还需要进行一次`resize`才能真正清除掉已经无效的后半部分，这个真正清除的过程被称为*擦除*（erase）；这个方法也被称为“删除-擦除”法。
]

非常显然，现在时间复杂度被缩减到$Theta(n)$了。上面这个算法的思路可以被概括为_快慢指针_，这是线性表算法设计中非常典型的技巧。快指针即探测指针，指向`V[r]`；慢指针即更新指针，指向`V[r-k]`。快指针找到需要保留的元素，然后将它们移动到慢指针的位置处。如@fig:vec9 所示。

#figure(
    image("images/vec9.svg", width: 60%),
    caption: "向量按值删除元素：快慢指针",
) <fig:vec9>

如@tab:vec3 所示，逐个查找的时间效率是最低的，而全局查找只有常数级优化。

#figure(table(
    columns: 4,
    [$n$], [逐个查找], [全局查找], [快慢指针],
    $10^3$, [0], [0], [0],
    $10^4$, [13], [9],  [0],
    $10^5$, [1425], [879], [0],
),
    caption: "向量按值删除算法的时间",
) <tab:vec3>

== 置乱和排序 <vec:shuffle-sort>

一般数据结构重点讨论的只有插入、删除和查找三种基本操作，但向量作为一种非常基础的数据结构，经常被用来承担一些辅助功能。下面这两个小节分别从_熵增_和_熵减_的角度出发，讨论*置乱*（shuffle）和*排序*（sort）的算法。

=== 随机置乱 <vec:shuffle>

通常说的置乱都是指随机置乱。给定一个向量`V`和一个随机数发生器`rand`，随机打乱向量中的元素。在理论分析的时候，可认为随机数发生器是_理想的_，即每次调用能够随机生成一个_非负整数_。//当然现实中的随机数发生器做不到理想，我们将在本小节的末尾讨论它们的区别。

置乱算法接收一个向量将它置乱。直接看这个“向量置乱”的问题，很容易没有头绪。不妨将这个问题迁移到比较熟悉的领域：比如洗牌。想必读者非常熟悉洗牌。随机置乱的目的和洗牌是一样的，但如果用洗牌的方法去做随机置乱，即抽出一沓牌、把这沓牌放到牌堆底部、再抽一沓牌，则会面临三个问题：
+ 您不知道重复多少次抽牌比较合理。
+ 在有限次抽牌之后，牌的`n!`种随机次序并不是等概率的。
+ 每次抽牌都要伴随大量的元素移动，时间效率非常低下。

#h(2em)
解决随机置乱问题可以从上面的第二个问题，也就是“随机次序等概率”入手。为了保证随机次序是等概率的，那么就要构造`n!`种等可能的情况。根据乘法原理，可以很自然地想到，如果将每种次序表示为一个`n`元随机变量组$(X_1, X_2, ..., X_n)$，其中$X_i$两两独立，并且$X_i$恰好有`i`个等可能的取值，那么这`n!`种次序就是等可能的了。接下来，只需要建立在全排列和这样的`n`元组的一一对应的映射关系即可。当然，不能直接把全排列用上。全排列的两个元素不是相互独立的，它自身不是符合条件的`n`元组。


为了构造符合条件的映射，又可以采用递归的思想方法：

+ 如果`n = 1`，全排列和`n`元组可以直接对应。
+ 对于`n > 1`，考虑`V[n-1]`在打乱后的秩，显然，它可以取`0,1,...,n-1`这$n$个等可能的值，令这个数为$X_n$，然后将`V[n-1]`从打乱前后的向量中都删除，就化为了`n-1`的情况。反复利用这个化归方法，最终可化归到`n = 1`的情况。

#h(2em)以上就成功构造出了满足条件的一一映射关系。

```cpp
void operator()(Vector<T>& V) override {
    for (auto i { V.size() - 1 }; i > 0; --i) {
        auto j { Random::get(i) };
        std::swap(V[i], V[j]);
    }
}
```

#h(2em)
显然上面这个算法是时间$Theta(n)$、空间$O(1)$的。并且上面的分析表明，如果`rand`真的能随机生成一个非负整数（不是随机生成一个`std::size_t`！），那么这个算法就能将所有的`n!`个排列等概率地输出。#linebreak()
#graybox[【C++学习】#linebreak()#h(2em)
C++11在STL中也提供了置乱算法，需要包含`<random>`库使用。
```cpp
void operator()(Vector<T>& V) override {
    std::shuffle(V.begin(), V.end(), Random::engine());
}
```
#h(2em)这里的随机数引擎可以被替换为其他用户定义的引擎，关于随机数引擎的问题和数据结构无关，不再赘述。默认的随机数引擎基于梅森旋转算法，产生随机数的速度较慢，但具有较好的均匀性。@fig:vec15 展示了4个元素随机置乱$10^6$次的结果，全部的24种可能情况出现的频率非常接近，平均相对误差只有大约0.4%。
]

#figure(
    image("images/vec15.svg", width: 100%),
    caption: "4个元素随机置乱的频率分布",
) <fig:vec15>

下面回到随机数生成器的问题上来，真实的`rand`会受到位宽的限制。
如果每次随机生成一个随机的32位非负整数，那么`n`次随机一共只有$2^(32n)=o(n!)$种可能的取值，所以在`n`充分大的时候，必然会有一些排列不可能被输出。

另一方面，即使忽略位宽的限制，也不可能做到等概率输出。因为当随机数生成器的返回值是在`[0,2^k-1]`中随机生成的非负整数时，`n!`在`n` $>=$ `3`时不是$2^k$的因子（不论`k`有多大），所以这`n!`个排列不可能是等概率的。不过，当`n`比较小时概率可以认为_近似_相等，您可以在示例程序的运行结果中看出这一点。

除此之外，现实中的`rand`通常是_伪随机_。对于同一个种子，生成的伪随机序列是相同的，所以并不能真正“随机”地打乱向量中的元素。当然，这是另一个话题了。当我们在后面的章节讨论散列的时候，再对伪随机问题进一步探讨。// TODO: 散列的时候再讨论伪随机问题。
#linebreak()
#linebreak()
#linebreak()


=== 偏序和全序 <vec:partial-total-order>

在讨论完置乱问题之后，接下来讨论排序问题。在具体介绍排序算法前，首先需要界定清楚，*序*（order）是一个什么东西。在@sum:correctness:well-order 定义过_良序_的概念，但要对一个向量做排序，并不一定要要求它的元素是某个定义了良序关系的类型。比如说，`n`个实数同样可以关于熟知的“$<=$”排序。因此，需要引入条件更松的序关系的定义。

将良序关系定义中的第4个条件（最小值）去掉，就变成了*全序*（total order）关系。
如果集合$S$上的一个关系$prec.eq$满足：

+ *完全性*。$x prec.eq y $和 $y prec.eq x$至少有一个成立。
+ *传递性*。如果$x prec.eq y$且$y prec.eq z$，那么$x prec.eq z$。
+ *反对称性*。如果$x prec.eq y$和$y prec.eq x$均成立，那么$x=y$。

#h(2em)
那么称$prec.eq$是$S$上的一个*全序关系*。显然良序关系是全序关系的子集。和良序关系相比，全序关系更加符合常规的认知。比如，实数集上熟知的“$<=$”就是全序关系。由于完全性的存在，凡是具有全序关系的数据类型，都可以进行排序；反之，在《数据结构》里通常讨论排序问题时，也总是假定数据结构中的元素数据类型具有全序关系。#linebreak()
#graybox[【C++学习】#linebreak()#h(2em)
在C++中，排序函数`std::sort`接受三个参数，其中第三个参数就表示“自定义的全序关系”。基本数据类型（如`int`和`double`），以及一些组合类型（如`std::tuple`）定义了内置的全序关系（即熟知的`operator<`），但也可以使用其他的全序关系进行排序。其他编程语言中的排序函数也有类似的设计。

传统上对于自定义类型通常通过重载`operator<`实现。C++20引入了航天飞机运算符`operator<=>`，定义该运算符之后会自动生成`operator<`、`operator>`、`operator<=`和`operator>=`，这样就可以方便地定义全序关系了。对于现实中遇到的大多数情况，都可以通过下面的方式定义全序关系：
```cpp
auto operator<=>(const T& other) const = default;
```
]

除了全序关系之外，还有一种序关系在《数据结构》中也经常会提到：*偏序*（partial order）关系。

如果集合$S$上的一个关系$prec.eq$满足：

+ *自反性*。$x prec.eq x $。
+ *传递性*。如果$x prec.eq y$且$y prec.eq z$，那么$x prec.eq z$。
+ *反对称性*。如果$x prec.eq y$和$y prec.eq x$均成立，那么$x=y$。

#h(2em)
那么称$prec.eq$是$S$上的一个*偏序关系*。偏序关系和全序关系相比，第1个条件（完全性）变成了更简单的自反性；也就是说，并不是$S$中的任意两个元素都能进行比较。比如，令$S$为“考生组成的集合”，$prec.eq$定义为“考生`x`的_每一门_分数都小于等于考生`y`”。显然这个关系是偏序关系但不是全序关系。

在计算机编程中直接定义偏序关系是不方便的，因为$prec.eq$的返回值往往是`bool`类型，不存在`true`和`false`之外的第三个选项（_无法比较_）。并且，无法比较的情况不能随意地返回一个`true`或`false`的值，因为这可能导致传递性被破坏。所以，当在编程时需要定义一个偏序关系时，往往会将它扩展成一个全序关系。比如，给$S$中的所有元素做标号，当已有的偏序关系无法比较时，则根据标号的大小进行比较。扩展成全序关系之后，就可以进行排序了。由扩展成的全序关系的不同，可能会产生不同的排序结果。#linebreak()
#graybox[【C++学习】#linebreak()
    #h(2em)
    在C++20中定义了各种序关系，用于作为`operator<=>`的返回值类型。比如，偏序关系被定义为`std::partial_ordering`，包括大于、小于、等价以及无法比较四个值。此外，还提供了包括大于、小于和等于的强序关系`std::strong_ordering`和包括大于、小于和等价的弱序关系`std::weak_ordering`。这两者的区别在于“等于”和“等价”，或者说“相同”和“相等”。强序关系不允许两个不同的元素相等（这是非常强的条件），而弱序关系允许。一般而言，全序关系即对应强序关系，而经过一个映射的全序关系（如二维坐标只比较一个维度） 则对应弱序关系。
]

=== 自上而下的归并排序 <vec:merge-sort>

现在回到向量排序的问题。对于一个线性表，如果它的数据类型是全序的；且对其中的任意一个元素`x`，和`x`的_后缀_中的任意一个元素`y`，总是有$x prec.eq y$，则称它是*有序的*（ordered）。对于无序线性表，通过移动元素位置使其变为有序的过程，称为*排序*（sort）。在计算机领域所说的有序，一般都是指_升序_。所以在上面的定义中使用的是_后缀_。如果您想要讨论降序或者其他的顺序（比如按最小素因子排序），只需要重新定义全序关系$prec.eq$，即可以回归为升序的情况进行处理。

```cpp
template <typename T, template<typename> typename L = DefaultVector>
    requires std::is_base_of_v<LinearList<T
                , typename L<T>::iterator
                , typename L<T>::const_iterator>, L<T>>
class AbstractSort : public Algorithm<void(L<T>&)> {
protected:
    std::function<bool(const T&, const T&)> cmp { std::less<T>() };
    virtual void sort(L<T>& L) = 0;
public:
    template <typename Comparator>
    void operator()(L<T>& L, Comparator&& cmp) {
        this->cmp = cmp;
        sort(L);
    }
    void operator()(L<T>& L) override {
        sort(L);
    }
};
```

// 在上面的程序中，我们规定了一个排序的抽象模板类，它接收一个线性表（通过概念限制）对其进行排序。因为排序的结果一定是一个序列，所以我们只考虑线性表。约定俗成地，我们通常使用严格的序关系，比如$<$，而不是不严格的$\le$。C++提供了模板\lstinline{std::less}来表示使用类型\lstinline{T}定义的小于运算符，用户也可以自定义仿函数传入这个模板中。

#h(2em)排序是计算机领域最重要的算法之一。在计算机出现至今，人们提出了各种各样的排序算法，并且仍然有不少研究者在从事着排序算法的研究。在《数据结构》中，将专门有一章讨论各种排序算法。在本节，先介绍一种最基本、最经典的排序方法：*归并排序*（merge sort）@knuth1997art 。归并排序的发明人是大名鼎鼎的冯·诺依曼（von Neumann），这位“计算机之父”在1945年设计并实现了该算法。 // TODO: 排序 章

#figure(
    image("images/vec10.svg", width: 60%),
    caption: "向量的归并排序",
) <fig:vec10>

归并排序的设计采用的仍然是递归的思想：
+ 规模$n <= 1$的向量总是天然有序的。
+ 对于规模$n > 1$的向量，可以将其分成前后两部分，长度分别为$n/2$和$n-n/2$（在@sum:correctness:array-sum 您见过这个分法），从而将规模为`n`的问题化归为两个规模较小的子问题。这些子问题可以继续递归下去直到化为1。解决子问题之后，`V`的前半部分和后半部分分别有序，只需要将这2个有序序列合并为1个有序序列，就可以解决原问题了。这一合并的过程就称为*归并*（merge）。

```cpp
void merge(iterator lo, iterator mi, iterator hi, std::size_t ls) {
    static Vector<T> W {};
    W.resize(ls);
    std::move(lo, mi, W.begin());
    auto i { W.begin() }, j { mi }, k { lo };
    while (i != W.end() && j != hi) {
        if (cmp(*j, *i)) {
            *k++ = std::move(*j++);
        } else {
            *k++ = std::move(*i++);
        }
    }
    std::move(i, W.end(), k);
}
void mergeSort(iterator lo, iterator hi, std::size_t size) {
    if (size < 2) return;
    auto mi { lo + size / 2 };
    mergeSort(lo, mi, size / 2);
    mergeSort(mi, hi, size - size / 2);
    merge(lo, mi, hi, size / 2);
}
void sort(L<T>& V) override {
    mergeSort(V.begin(), V.end(), V.size());
}
```

#graybox[【C++学习】#linebreak()#h(2em)
    C++的STL提供了`std::inplace_merge`函数（传入`lo`、`mi`和`hi`的迭代器）来进行归并，这个函数可以用来替代上述实现中的`merge`。对于向量来说，左半部分的长度`ls`可以通过`std::distance(lo, mi)`得到，所以也可以不用传入`ls`。
]

我们注意到在归并的一开始，将排序好的前半部分移动到了辅助向量中。那么，为什么前半部分需要移动出去，而后半部分不需要呢？
这是因为在归并的过程中，事实上也采用了_快慢指针_的思想。快指针（探测指针）是`j`，慢指针（更新指针）是`k`。还有一个探测指针是`i`，不过它工作在辅助向量`W`上。
如果前半部分不移动的话，`i`也会工作在原向量上，它和`k`不构成快慢关系。于是，一旦后半部分比前半部分的元素小，就会把前半部分的元素覆盖掉；而对后半部分而言，除非前半部分已经全部加入到原向量中，否则`j`永远能够在`k`前面，快慢关系是始终能保持的，不会有元素被错误地覆盖掉。而当前半部分全部加入之后，`i`到达了辅助向量`W`的末尾，循环结束。

另一方面，在归并的最后，我们将前半部分（此时在辅助向量里）多余的元素移动回原向量。为什么后半部分多余的元素不需要呢？这是因为后半部分的数据没有移动出去，如果前半部分的元素已经全加入到原向量了，则后半部分剩余的元素已经在它们应该在的位置上，不需要再移动了。

辅助向量`W`的长度至少为最大的`ls`，也就是$n/2$，因此空间复杂度为$Theta(n)$。
递归产生的$Theta(log n)$，相比于辅助数组的$Theta(n)$来说可以忽略。归并排序的时间复杂度则是一个非常典型的问题。容易发现`merge`的时间复杂度为$Theta(n)$（这里假定`cmp`是$O(1)$的），因而可以得到递归式$T(n) = 2T(n/2) + Theta(n)$。此类递归式的求解通常可以用*主定理*（master theorem）@bentley1980general 解决。主定理是用来处理分治算法得到的递归关系式的“神兵利器”。它的证明太过复杂@cormen2022introduction ，这里只陈述结论。

设$T(n) = a T(n/b) + f(n)$，则：
+ 若$f(n) = O(n^(log _b a - epsilon.alt))$，其中$epsilon.alt > 0$，则$T(n) = Theta(n ^ (log _b a))$。
+ 若$f(n) = Theta(n^(log _b a))$，则$T(n) = Theta(n ^ (log _b a)log n)$。
+ 若$f(n) = Omega(n^(log _b a + epsilon.alt))$，其中$epsilon.alt > 0$，且$lim sup_(n arrow.r infinity) f(n/b)f(n)<1$，则$T(n) = Theta(f(n))$。

#h(2em)
根据主定理可以得到归并排序的时间复杂度为$Theta(n log n)$。

=== 自下而上的归并排序 <vec:merge-sort-bottom-up>

上一节中展示的归并排序是自上而下的。这里的“自上而下”指的是，我们首先调用整个向量的归并排序，在这个函数中递归地调用子向量的归并排序。以此类推，直到“最下方”的递归实例，也就是递归边界（只有一个元素的向量）。

自上而下的归并排序是归并排序的经典实现，下面介绍一种自下而上的方法。我们分析归并函数`merge`的调用次序会发现，由于归并发生在递归调用之后，所以归并的次序反而是自下而上的：首先对“最下方”的子向量做归并，得到长度为2的有序子向量。一个子向量可以被归并，当且仅当它的左半和右半的子向量已经被归并完成。

#figure(
    image("images/vec14.svg", width: 90%),
    caption: "自上而下和自下而上的归并次序",
) <fig:vec14>

如@fig:vec14 中左图所示，在自上而下的归并排序中，只要左半和右半的子向量都已经被归并完成，就会引发这两个子向量的归并。对于7个元素组成的向量，一共会触发6次归并，归并的次序在图中用1至6表示。由于一个向量的归并，只需要保证它左半和右半归并完成之后进行，而并不要求在左半和右半归并完成之后_立即_进行，所以我们可以调整这6次归并的次序。我们首先将向量视作长度为1的段两两合并，然后将它视作长度为2的段（每个段内已经有序）两两合并，再视作长度为4的有序段两两合并，以此类推，直到归并整个向量为止。上面的每一步称为一*趟*（run）。这种方法同样保证了一个子向量晚于它的左半和右半被归并，因而也能保证正确性。

如@fig:vec14 所示，因为向量规模不一定是2的幂次，每一趟的末尾处需要特殊处理。从图中还可以看出，自下而上的归并排序对于子向量的分拆方式，是有可能和自上而下不同的。
下面展示了自下而上的归并排序的一个实现，它复用了自上而下版本的`merge`函数，只是在调用`merge`的次序上和自上而下的版本有所区别。

```cpp
void sort(L<T>& V) override {
    auto n { V.size() };
    for (auto w { 1uz }; w < n; w *= 2) {
        auto lo { V.begin() };
        auto i { 2 * w };
        while (i < n) {
            auto mi { lo + w };
            auto hi { mi + w };
            merge(lo, mi, hi, w);
            lo = hi;
            i += 2 * w;
        }
        if (n + w > i) {
            auto mi { lo + w };
            auto hi { V.end() };
            merge(lo, mi, hi, w);
        }
    }
}
```

#h(2em)
外层循环进行的次数（也就是趟数）很显然是$ceil( log n )$，每一趟的时间复杂度为$Theta(n)$。因此，自下而上的归并排序时间复杂度同样是$Theta(n log n)$。

自下而上的归并排序的时间消耗随$n$的增长是不平滑的。当$n$从$2^k$增长到$2^k+1$时，自下而上的归并排序需要增加整整一趟；因此，它在$n$略小于或等于一个2的幂次的时候表现更好，而在略大于一个2的幂次时表现较差。

=== 基于比较的排序的时间复杂度 <vec:comparison-sort>

归并排序是一种*基于比较*（comparison-based）的排序。
所谓基于比较，就是在算法进行过程的每一步，都依赖于元素的比较（即调用`cmp`）进行。基于比较的排序是针对_全序关系_设计的。大多数的排序算法都是基于比较的。
还有一些不基于比较的排序，它们不是针对待排序数据类型的全序性设计的，而是针对待排序数据类型的其他性质设计的，因而应用范围会更小。在后文中会介绍一些不基于比较的排序。

下面将说明一个重要结论：基于比较的排序在最坏情况下的时间复杂度为$Omega(n log n)$。

这是本书中第一次使用信息论方法，讨论时间复杂度的_最优性_。使用信息论的思路，有助于在算法设计的过程中辅助自己判断是否达到了最优的时间复杂度，也有助于记忆知识点。

// \begin{enumerate}
+ 因为是最坏情况，不妨假设所有元素互不相等。在排序算法开始之前，这`n`个元素可能的顺序关系有`n!`种，而在排序算法结束之后，这`n`个元素可能的顺序关系只有1种（因为已经找到了它们的顺序）。
+ 另一方面，每次比较都有两种结果（`if`分支和`else`分支）。剩下的可能的顺序关系被分为2个部分，根据比较结果，只保留其中的1个部分。在最坏情况下，每次保留的都是元素较多的部分，从而每次比较至多排除一半的可能。
// \end{enumerate}

#h(2em)
综合以上两点，至少需要进行$log_2(n!)=Theta(n log n)$次比较，于是就证明了上述的定理。最后一步的结论基于一个重要的公式：

// \begin{theorem}[斯特林公式]

*斯特林公式*：在$n arrow.r infinity$时，$n! tilde.op sqrt(2 pi n) dot (n/e)^n$。
// \end{theorem}

这一公式的证明是纯数学的话题。感兴趣的话可自行在网上查找，这里不再叙述。
在《数据结构》的学习中需要记住的公式并不多，斯特林（Stirling）公式是必须记住的公式之一。或者，您也可以只记住$log(n!)=Theta(n log n)$，因为这是它的主要应用。

由此可见，归并排序在基于比较的排序中，已经达到了最优的时间复杂度。当然，空间复杂度不是最优的，它需要$Theta(n)$的额外空间。关于排序的更多性质，在后面的专门章节中将继续分析。从这个时间复杂度下界也可以看出，排序需要付出的努力总是比置乱要高，这也符合我们对信息的一般认知：排序是熵减的过程，而置乱是熵增的过程，熵减总是要付出更多的努力。 // TODO: 排序章

=== 信息熵 <vec:information-entropy>

基于@vec:comparison-sort 的讨论，本小节对香农（Shannon）提出的*信息熵*（information entropy）概念进行简要的介绍。// 在使用信息论判断复杂度问题时，并不一定需要使用信息熵去定量计算，因此如果您不感兴趣，也可以跳过本节。

在信息熵提出之前，人们很难定量地衡量一份信息所包含的_信息量_。香农创造性地引入概率论和热力学中的熵的概念，对信息的多少进行了定量描述。对于一个信息来说，在我们识别之前，会对它有一些先验的了解。如果我们已经先验地确切知道这个信息的内容，那么这个信息就完全是无效信息，所蕴含的信息量是0；反过来，我们对这个信息的先验了解越少、越模糊，这个信息所蕴含的信息量越丰富。

假设一个信息在我们已知的先验了解下，共有`n`种可能发生的情况。则对于每个情况，设其发生的概率`p`，我们定义该情况的不确定性为$- log p$。
对于一个信息整体，我们考虑它所有可能发生的情况的_平均不确定性_（即数学期望），定义其为该信息的信息熵，即：
$
H = E(-log p) = -sum_(i=1)^n p_i log p_i
$

#h(2em)
现在联系@vec:comparison-sort 讨论的场景。在没有其他先验信息的情况下，一个乱序序列出现`n!`种排列的可能性是相等的，所以它的信息熵为：

$
H = -log (1/n!) = Theta(n log n)
$

#h(2em)在排序结束时，序列只有唯一的可能性，所以信息熵为0。
在最坏情况下，我们进行1次比较-判断可以最多消除一半的不确定性，由于取了对数，所以一次判断能造成的熵减是$O(1)$的。综上所述，基于比较的排序的最坏时间复杂度是$Omega(n log n)$的。

需要注意的是，即使信息熵的初值和终值都为0，也不代表可以在$O(1)$的时间内完成算法。比如，对于完全倒序的序列来说，它的信息熵也为0，但是将其进行排序需要$Theta(n)$的时间对序列进行倒置。所以，信息熵方法通常只能求出一个理论边界，并不代表这个边界是可以达到的。

=== 有序性和逆序对 <vec:order-and-inversion>

@vec:comparison-sort 和@vec:information-entropy 的讨论显示，
对于没有任何先验信息的乱序序列来说，它的信息熵是$Theta(n log n)$的，因此基于比较的排序在最坏情况下，永远不可能突破这一时间复杂度限制。但是，如果我们事先了解到了一些先验信息，初始状态的信息熵就会下降，从而在理论上可以以更低的时间复杂度进行排序。

一个常见的情况是“基本有序”的条件。对于基本有序，通常有两种理解方式。

+ 认为基本有序就是信息熵很低的状态。我们知道，信息熵对应了信息的不确定性，也就是“无序性”，信息熵比较高的序列无序性也比较高。因此，反过来也可以认为，信息熵比较低的序列基本有序。
+ 认为基本有序指的是*逆序对*很少的状态。对于一个序列`A[0:n]`，如果`i<j`但$A[i] succ A[j]$，则称$(i,j)$是一个逆序对。采用逆序对对序列有序性进行刻画，和信息论方法是分离的（因为一次交换可以最多消除$Theta(n)$个逆序对），但可以对顺序和倒序进行区分，在分析算法时常常可以起到重要的作用。

#h(2em)信息熵和逆序对都是我们分析和解决排序问题的方法。信息熵的视角更加宏观，我们很难计算每一步操作削减了多少信息熵，因此它往往用于复杂度层面上的分析；而逆序对的视角更加微观，很容易计算每一步操作消除了几个逆序对，所以可以用于具体的算法性质分析。
下面我们从逆序对的角度回顾归并的过程。每次向更新指针的位置移动一个元素：
+ 如果被移动的元素来自于前半部分的探测指针，那么不会对逆序对的数量产生影响。
+ 如果被移动的元素来自于后半部分的探测指针，那么我们消除了它和前半部分剩余元素之间的逆序对。也就是说，我们消除的逆序对数量等于前半部分的剩余元素数量。

#h(2em)根据这一性质，我们可以在归并排序的过程中统计逆序对的数量。因为在前半和后半之一的元素被用尽之后，后半元素不需要移动，而前半元素需要从辅助空间中移回；所以，上面讨论的情况（2）会比情况（1）有数量更多的移动。因此我们可以看出，归并排序在处理完全倒序的序列时，尽管它的信息熵为0，但排序算法并不能发现这一先验信息，而是会进行更多的移动。

=== 先验条件下的归并排序 <vec:merge-sort-prior>

#bluetxt[实验`vmergesort.cpp`。]
下面讨论一个基本有序的场景：如果向量已经基本有序，只有开头的长度为$L$（未知）的一小段前缀是乱序的（即前缀外全部有序，且前缀中的元素都比前缀外的元素小），如何改进我们的归并排序，让它可以有更高的时间效率？改进之后的时间复杂度是多少？

这个问题也是一个排序经典问题。我们可以用信息熵和逆序对两种方法对这个场景进行分析。我们可以发现在这个场景中，乱序前缀之外的所有元素既不会提供信息熵也不会提供逆序对，因此在给定的先验条件下，序列的信息熵为$Theta(L log L)$，逆序对数量为$O(L^2)$。而原有的归并排序算法在最好情况下，时间复杂度也是$Theta(n log n)$的。所以必须要改进。

改进的时候，可以针对已知的方程$T(n)=2T(n/2)+Theta(n)$做优化。
最直接的想法是从递归方程的目标入手，也就是将$T(n)$替换为$T(L)$。从信息熵和逆序对的角度我们都可以发现，问题中的特殊场景相当于把排序问题的规模从$n$降低到了$L$。我们可以从后向前遍历整个向量，以确定$L$的值。这种方法看起来非常直观，但确定$L$并没有那么简单，以下展示了一种可能的实现。

```cpp
void sort(L<T>& V) override {
    auto mi { --V.end() };
    while (mi != V.begin() && cmp(*(mi - 1), *mi)) {
        --mi;
    }
    auto max_left { *std::max_element(V.begin(), mi) };
    auto left { std::lower_bound(mi, V.end(), max_left, cmp) };
    mergeSort(V.begin(), left, std::distance(V.begin(), left));
}
```

#h(2em)
另一个思路是从递归方程的形式入手。注意到，在这个方程中，递归项$2T(n/2)$只要不改动递归方式，就是没法做优化的；而余项$Theta(n)$是有机会被优化的。需要在“比较好的情况”（即题中给出的“基本有序”的情况）下，让$Theta(n)$变得更小。归并排序在“将前半部分移动到辅助空间”的时候，就已经需要付出$Theta(n)$的时间。所以，我们需要在归并的最开始进行一次判断，判断是否可以不将前半部分移动到辅助空间：只需要判断前半部分的结尾是否小于后半部分的开头就可以。在我们之前实现的归并算法中，只需要加入下面的一行代码：
```cpp
if (cmp(*(mi - 1), *mi)) return;
```

#h(2em)这样，如果归并前的序列已经有序，就不需要进行归并。于是，对于不需要归并的部分，递归方程就变化为$T(n) = 2T(n/2) + O(1)$，也就是$Theta(n)$。而需要归并的部分由题意，长度不超过$L$，在这部分利用前面获得的结论，就可以得到时间复杂度为$Theta(L log L)$。
因此，改进后的时间复杂度为$Theta(n+L log L)$。

由于归并排序的实际时间性能还和很多其他的因素相关，所以本书提供的实验代码只能大致地进行定性分析。如果您想要得到更加精准的分析结果，应当多次随机取平均值。我们可以从实验结果中看到，在题目给定的条件（前缀乱序）下，两种改进策略的时间性能差不多；同时在$L$不大的时候，两种改进策略的性能都显著高于未改进的版本。// 您可以自己用上面介绍的两种方法，对自下而上的归并排序进行修改，使其时间复杂度达到$Theta(n+L log L)$，并加入实验框架进行比较。

#figure(
    image("images/vec16.svg", width: 80%),
    caption: "归并排序在基本有序的情况下的性能比较",
) <fig:vec16>

如 @fig:vec16 所示，在上述两种方法中，从递归方程形式入手的方法具有更好的泛用性。右图测试了一个对偶的问题：有且只有后缀是乱序的问题。在这个对偶问题上，从递归方程入手的方法仍然可以做到高效率，而从递归目标入手的方法则因为无法识别先验信息，和未改进的版本性能相若。从图中还可以看出，即使是没有做针对性优化的版本，由于有序段内不需要移动后半段，在“整体有序”的情况下性能也可以得到一定的提升。

=== 原地归并排序 <vec:in-place-merge-sort>

此前所展示的归并排序都是需要辅助空间的，在归并的过程中，需要将前半部分的元素移动到辅助空间中。这样的归并排序称为*非原地*（non-in-place）归并排序。在实际应用中，非原地归并排序的空间消耗是一个潜在的问题。在这一节中，我们介绍一种*原地*（in-place），即空间复杂度$O(1)$的归并排序的方法。

为了降低空间复杂度，我们有两种思路：
+ 直接降低*归并*的空间复杂度，即在合并两个长度为`n`的有序序列时，不再需要$Theta(n)$的辅助空间。这一思路是相当困难的 @knuth1997art 。
+ 不降低*归并*的空间复杂度，即在合并有序序列的时候仍然使用辅助空间，但是让这个辅助空间“羊毛出在羊身上”，利用原向量中的空间作为辅助空间使用。这一思路的基础是：当我们将前半部分的元素移动到辅助空间后，原有的空间是空出来的；而当我们完成归并之后，辅助空间又被空了出来。也就是说，辅助空间并不一定需要是实际新开辟出来的一块空闲的空间，它可以原先存放一些数据。当我们将前半部分的元素移动到辅助空间时，辅助空间原先存放的数据也被交换到了前半部分。而随着归并的进行，这些数据又被移回了辅助空间里。

#figure(
    image("images/vec18.svg", width: 50%),
    caption: "原地归并排序",
) <fig:vec18>

在经典的自上而下的归并排序中，最后一趟会归并两个长度为$n/2$的向量，需要长度为$n/2$的辅助空间，但这个时候原向量没有可以“薅羊毛”的空间了，因此无法使用上面的思想原地归并。但容易想到的是，归并所消耗的空间是*不对称*的，前半部分越长，所需的辅助空间越大，而后半部分的增长并不会影响对辅助空间的需求量。为此，我们可以改变分治的方式，不再采用二等分的形式，而是采用*不对称*的分法，如 @fig:vec18 所示。我们先取$n/2$长的向量作为“后半部分”递归排序，再取$n/4$长的向量作为“前半部分”递归排序；此时，剩余的$n/4$长的未排序部分，恰好能作为足以容纳前半部分的辅助空间。于是，我们可以顺利进行归并，得到$n/4$长的未排序部分，以及$3/4n$长的已排序部分。接下来，再从未排序部分中取出一半$n/8$递归排序并作为“前半部分”，和已排序的$3/4n$归并，此时剩余的$n/8$又可以被当做辅助空间使用，以此类推。当分治算法的各部分对时间或空间的使用量不对称时，顺应这个不对称性，采用不对称的分法，是优化分治算法的重要思路。

```cpp
void merge(iterator lo, iterator mi, iterator hi, iterator tmp) {
    std::swap_ranges(lo, mi, tmp);
    auto i { tmp }, j { mi }, k { lo };
    auto te { tmp + std::distance(lo, mi) };
    while (i != te && j != hi) {
        if (cmp(*j, *i)) {
            std::iter_swap(k++, j++);
        } else {
            std::iter_swap(k++, i++);
        }
    }
    while (i != te) {
        std::iter_swap(k++, i++);
    }
    while (j != hi) {
        std::iter_swap(k++, j++);
    }
}
void mergesort(iterator lo, iterator hi) {
    if (std::distance(lo, hi) < 2) return;
    auto mi2 { hi - std::distance(lo, hi) / 2 };
    mergesort(mi2, hi);
    while (std::distance(lo, mi2) > 1) {
        auto mi1 { mi2 - std::distance(lo, mi2) / 2 };
        mergesort(mi1, mi2);
        merge(mi1, mi2, hi, lo);
        mi2 = mi1;
    }
    merge(lo, mi2, hi, W.begin());
}
```

#h(2em)如上所述，只有最后一次`merge`的时候，因为两个待归并向量已经填满了整个向量，没有多余的辅助空间了，所以需要使用`W`作为辅助空间。但是我们的减半策略使得最后一次`merge`的时候，前半部分的长度必定为1，因此只需要$O(1)$的辅助空间。这样，我们就实现了原地归并。

以上述方法实现的原地归并排序保持了原有归并排序的时间复杂度，但是存在两个方面的问题。首先，如 @fig:vec18 所示，在原地归并的过程中，用作辅助空间的区域`L`有可能会被直接交换到左侧（`M`中元素小的情形），也有可能先被交换到右侧若干次（`R`中元素小的情形），再交换到左侧。这种不确定性导致原地归并之后的`L`和归并前的`L`不再是相同的，而是会被打乱次序。当然，如果向量中的元素两两不相等，那么排序的最终结果总是正确的；问题在于，如果向量中存在两个相等的元素$a_1,a_2$，且$a_1$位于$a_2$之前，那么以此法排序之后，$a_1$是有可能位于$a_2$之后的。这种不能保持相等元素的相对次序的排序称为*不稳定*（unstable）排序，相应地，如果相等元素的相对次序不变，那么称为*稳定*（stable）排序。此前介绍的自上而下或自下而上的归并排序都属于稳定排序，而从上面的分析可以知道原地归并排序是不稳定的。其次，原地归并排序的空间复杂度虽然是$O(1)$，但是归并排序中的移动（move）操作变成了交换（swap），时间常数大幅提高。由于这两方面的原因，原地归并排序的实际应用范围有限。

== 有序向量上的算法 <vec:sorted-algorithm>

=== 折半查找 <vec:binary-search>

排序之后得到的有序向量，在查找时有额外的优越性。排序之后要执行查找操作，就不再需要一个一个元素看是否相等了。类似排序，我们首先建立针对有序线性表的抽象查找类。

```cpp
template <typename T, template<typename> typename L = DefaultVector>
    requires std::is_base_of_v<LinearList<T
                , typename L<T>::iterator
                , typename L<T>::const_iterator>, L<T>>
class AbstractSearch : public Algorithm<
                  typename L<T>::iterator(const L<T>&, const T&)> {
protected:
    using iterator = typename L<T>::iterator;
    std::function<bool(const T&, const T&)> cmp { std::less<T>() };
    virtual iterator search(const L<T>& V, const T& e) = 0;
public:
    template <typename Comparator>
    iterator operator()(const L<T>& V, const T& e, Comparator&& cmp) {
        this->cmp = cmp;
        return search(V, e);
    }
    iterator operator()(const L<T>& V, const T& e) override {
        return search(V, e);
    }
};
```

#h(2em)这里可以使用刚才介绍的，基于比较的算法思路。将被查找的元素`e`和向量中的某个元素`V[i]`比较，比较结果有2种：
+ 如果`V[i] > e`，那么只需要保留`V[0:i]`作为新的查找区间。
+ 如果`V[i] <= e`，那么只需要保留`V[i:n]`作为新的查找区间。

#h(2em)当取$i = n/2$（*折半*）时，可以保证新的查找区间长度大约是原来的一半，从而在$Theta(log n)$的时间里完成查找。所以这个思路称为*折半查找*。当然，也存在其他二分的方法（`i`取其他值），参见后面的《查找》一章。下面给出了一个使用递归的示例代码，它实现了刚才的设计。// TODO:，参见后面的《查找》一章。

```cpp
iterator search(const Vector<T>& V, const T& e
              , iterator lo, iterator hi) const {
    if (std::distance(lo, hi) <= 1) {
        if (lo != V.end() && cmp(*lo, e)) {
            return hi;
        } else {
            return lo;
        }
    }
    auto mi { lo + std::distance(lo, hi) / 2 };
    if (cmp(e, *mi)) {
        return search(V, e, lo, mi);
    } else {
        return search(V, e, mi, hi);
    }
}
```

#h(2em)上面的算法，时间复杂度和空间复杂度均为$Theta(log n)$。查找是算法设计的重点。在设计的时候，需要尤其注意多个相等元素的时候是返回秩最大、秩最小还是任意一个，以及查找失败的时候返回何种特殊值。如果是无序向量，正如 @vec:find 那样，那么在查找失败的时候很自然地会返回一个无效值，比如说`V.end()`。但是在有序向量的情况下，即使查找失败，我们也可以返回一些_有意义_的值，向调用者传递一些额外的信息。下面以前面的折半查找算法为例，分析查找成功和查找失败的情况下返回值的设计。

因为如果`e < *mi`就只保留前半段（`lo`到`mi`的部分），所以我们在任何时刻都可以保证，`e < *hi`（可认为初始值`*V.end() = `$+infinity$）。另一方面，因为另一边的比较是不严格的，所以我们保证的是`*lo <= e`；注意由于`lo`的初始值是有意义的`V.begin()`，不像`hi`的初始值是无意义的`V.end()`，所以后面这个式子只有`*lo != V.begin()`也就是`lo`被修改过一次之后才成立。

而当进入递归边界的时候，从`lo`到`mi`之间有且只有一个元素`*lo`。此时，可以分成小于、等于、大于三种情况讨论。
+ 如果`*lo < e`，那么我们可以定位到`*lo < e < *hi`，此时返回`hi`，就是大于`e`的第一个元素。
+ 如果`*lo = e`，那么我们直接返回`lo`，符合查找算法的期待；并且，由于`e < *hi`，所以如果有多个等于`e`的元素，则返回的是最大的一个。
+ 如果`*lo > e`，根据前面的分析可知，这种情况只可能发生在`lo = V.begin()`的情况。于是，`e`小于向量中的所有元素，此时返回`lo`，也是大于`e`的第一个元素。

#h(2em)综上所述，我们验证了上述折半查找算法的正确性，并且在查找成功时，返回的是最大的秩，在查找失败时，返回的是大于`e`的第一个元素的秩。二分算法在设计的时候非常容易出错；当您自己设计二分算法的时候，也可以使用上面的思考流程来分析自己的算法。

=== 消除尾递归 <vec:tail-recursion>

查找`V[0:n]`中某个元素`e`的位置，这个问题在计算前有$n+1$种（包括不存在情形的`V.end()`）可能的结果，计算后有1种确定的答案，因此从信息论的角度讲，最坏时间复杂度一定是$Omega(log n)$的。但空间复杂度并不一定要是$Omega (log n)$。在这一小节，将介绍一种叫做*消除尾递归*的技术，使用这个技术，可以将上面的折半查找算法的空间复杂度降为$O(1)$。

如果一个递归函数只在返回（return）前调用自身，则称其为*尾递归*（tail recursion）。在实际的编程过程中如果开启了编译器优化选项，则尾递归在通常会被编译器自动优化。

本小节只介绍对于尾递归的消除方法，其他类型的递归消除将在后文中讨论。
对于尾递归，只需要将递归函数的参数作为循环变量，就可以将其改写为不含递归的形式，从而降低空间复杂度。下面的模板是典型的尾递归。 // TODO: 后文中讨论

```cpp
T recursive(Args... args) override {
    if (is_base_case(args...)) {
        return base_case(args...);
    } else {
        next_args(args...);
        return recursive(args...);
    }
}
```

#h(2em)上面的递归算法可以改写成如下的迭代算法：
```cpp
T iterative(Args... args) override {
    while (!is_base_case(args...)) {
        next_args(args...);
    }
    return base_case(args...);
}
```

#h(2em)上面这种形式是非常通用的，`is_base_case`是判断是否到达递归边界的函数，`base_case`是递归边界的处理函数，`next_args`是递归函数的参数更新函数。下面给出了折半查找的迭代形式。

=== 迭代形式的折半查找 <vec:iterative-binary-search>

#bluetxt[实验`vsearch.cpp`。]
将折半查找的递归形式进行拆解，分解出`is_base_case`、`next_args`和`base_case`三个函数，然后代入到上面的迭代模板中，就可以将其改写为迭代形式。下面是迭代形式的一个示例程序，它和最初的递归形式是完全等价的。

```cpp
iterator search(const Vector<T>& V, const T& e) override {
    auto lo { V.begin() }, hi { V.end() };
    while (std::distance(lo, hi) > 1) {
        auto mi { lo + std::distance(lo, hi) / 2 };
        if (cmp(e, *mi)) {
            hi = mi;
        } else {
            lo = mi;
        }
    }
    if (lo != V.end() && cmp(*lo, e)) {
        return hi;
    } else {
        return lo;
    }
}
```

#h(2em)在示例的测试程序中，我们构造了长度为`n`的向量，将其随机赋值为某个区间的数并排序，然后重复$10^7$次查找（这是因为单次查找的效率太高，无法正确反映各个算法的性能差异）。我们会发现，由于进行了一些抽象，使用模板的方法性能会低于直接递归或迭代的性能。但即使是模板，它作为对数复杂度的算法，效率仍然远远高于在 @vec:find 中讨论的顺序查找。

#figure(table(
    columns: 5,
    [$n$], [递归], [迭代], [递归（模板）], [迭代（模板）],
    $10^4$, [317], [169], [479], [440],
    $10^5$, [401], [186], [499], [500],
    $10^6$, [511], [226], [548], [578]
),
    caption: "向量按值删除算法的时间",
) <tab:vec4>

现代C++编译器如果开启了优化选项，通常可以在编译的过程中自动消除尾递归，所以在实际上机编程时，通常不需要刻意将尾递归改写成循环形式。但是如 @tab:vec4 所示，递归版本的性能仍然可能和迭代版本有一定差异（和编译器相关），所以简单的尾递归改写成循环形式仍然是有益的。此外，一些语言（比如Python）拒绝提供尾递归优化 @van2012tail，在C++中运行良好的代码移植到这些语言上可能会发生问题，因此手动将尾递归改写成迭代形式仍然是非常重要的技能。

=== 向量唯一化 <vec:unique>

#bluetxt[实验`vunique.cpp`。]下面讨论*唯一化*（unique）问题。给定一个向量，我们希望删除它中间_相等_的重复元素，只保留秩最小的那一个。这里再次看到了_相等_和_相同_的区别，数据结构中是不可能有相同的元素的，而相等的元素我们可以用它的地址（迭代器记录的位置）来区分。

一个简单但有效的解法是：从左到右考察向量`V`中的每个元素，删除这个元素的后缀中，所有和它相等的元素。

```cpp
void operator()(Vector<T>& V) override {
    for (auto i { V.begin() }; i != V.end(); ++i) {
        auto j { i };
        while (j = std::find(i + 1, V.end(), *i), j != V.end()) {
            V.erase(j);
        }
    }
}
```

#h(2em)上面的方法是比较基本的逐个删除。利用了我们在 @vec:remove-value 中使用的“删除-擦除”法一次性地删除后缀里的所有重复元素，可以对上面的算法做出简化。

```cpp
void operator()(Vector<T>& V) override {
    for (auto i { V.begin() }; i != V.end(); ++i) {
        V.resize(std::remove(i + 1, V.end(), *i) - V.begin());
    }
}
```

#h(2em)在最好情况下（所有元素都相等），“删除-擦除”法可以达到$Theta(n)$的时间复杂度。虽然按值删除达到了$Theta(n-r)$的时间复杂度，但是因为`r`需要从0到`n`地遍历整个向量，所以在最坏情况下需要进行$Theta(n^2)$次比较。

值得一提的是，如果按值删除的时候采用的是最坏情况$Theta(n^2)$的朴素（逐个删除）算法，那么唯一化在最坏情况下时间复杂度仍然是$Theta(n^2)$，并不会来到$Theta(n^3)$。这是初学者容易出现的错误，即直接把循环内外的时间复杂度相乘。这种天真的想法可能是来自于公式$O(f(n)) dot O(g(n)) = O(f(n) dot g(n))$。这个公式本身没有问题，但我们联系一下概率论里的$P(A)P(B)=P(A B)$的条件就能发现端倪：如果内层循环和外层循环是有联系的（不独立的），那么就不能直接相乘。比如，在上面的代码中，外层循环的迭代器是`V.begin()`到`V.end()`；然而，`std::distance(V.begin(), V.end())`并不保持是一个常量`n`，在内层循环中它会发生变化。在使用朴素算法进行按值删除的时候，删除的元素越多，花费的时间越长，但同时缩小的向量规模也就越多，减少的外层循环的轮数也就越多。

在有序向量的情况下，问题可以得到进一步的简化。相等的元素总是排在连续的位置的。所以，可以通过快慢指针的方法。快指针一次经过一片相等的元素，而慢指针保留这些元素中的第一个（这个算法的前提条件并不是有序，只要求相等的元素排在一起即可，但通常建立这一条件的方法是排序）。

```cpp
void operator()(Vector<T>& V) override {
    auto sp { V.begin() }, fp { V.begin() };
    while (++fp != V.end()) {
        if (*sp != *fp) {
            *++sp = std::move(*fp);
        }
    }
    V.resize(++sp - V.begin());
}
```

#graybox[【C++学习】#linebreak()#h(2em)
在STL中，提供了`std::unique`来进行唯一化操作，它同样要求相等的元素排在一起。`std::unique`和`std::remove`一样不提供擦除功能，需要再进行`resize`。
]


这个算法的时间复杂度是$Theta(n)$的。
如果定义了全序关系（即可排序），那么无序向量的唯一化可以化归到有序向量的情况进行处理：先进行一次$Theta(n log n)$的排序，再用有序向量唯一化。但在排序的时候，会损失“元素原先的位置”这一信息，所以需要开辟一个额外的$ Theta(n)$的空间保存这一信息，以在唯一化之后能够顺利还原。它的时间复杂度为$Theta(n log n)$，优于前面的逐个“删除-擦除”的做法；但需要引入辅助向量，空间复杂度为$Theta(n)$。

```cpp
template <typename T>
class VectorUniqueSort : public VectorUnique<T> {
    struct Item {
        T value;
        std::size_t rank;
        bool operator==(const Item& rhs) const {
            return value == rhs.value;
        }
        auto operator<=>(const Item& rhs) const {
            return value <=> rhs.value;
        }
    };
    Vector<Item> W;
    void moveToW(Vector<T>& V) {
        W.clear();
        std::transform(V.begin(), V.end(), std::back_inserter(W), 
            [](const T& e) { return Item { e, 0 }; });
    }
    void moveFromW(Vector<T>& V) {
        V.clear();
        std::transform(W.begin(), W.end(), std::back_inserter(V), 
            [](const Item& item) { return item.value; });
    }
public:
    void operator()(Vector<T>& V) override {
        moveToW(V);
        std::sort(W.begin(), W.end());
        W.resize(std::unique(W.begin(), W.end()) - W.begin());
        std::sort(W.begin(), W.end(), 
            [](const Item& lhs, const Item& rhs) {
            return lhs.rank < rhs.rank;
        });
        moveFromW(V);
    }
};
```

#h(2em)如 @fig:vec17 所示，通过实验我们看到，在最坏情况下（所有元素互不相同），直接对无序向量进行处理，无论采用“删除-擦除”法还是逐个删除法都非常慢，而先排序再复原则快得多。而在最好情况下（所有元素都相同），“删除-擦除”法时间复杂度仅为$Theta(n)$最快，先排序后复原需要$Theta(n log n)$较慢，而逐个删除的方法仍然需要$Theta(n^2)$最慢。

#figure(
    image("images/vec17.svg", width: 100%),
    caption: "最坏和最好情况下的向量唯一化性能",
) <fig:vec17>


== 循环位移 <vec:rotate>

#bluetxt[实验`vrotate.cpp`。]
通常我们遍历向量都采用从前向后、一个一个元素遍历的形式。
在本章的最后，用*循环位移*（cyclic shift）作为例子，讨论一下非常规的遍历方法。

给定向量`V[0:n]`和位移量`k`，则将原有的向量`V[0],V[1],...,V[n-1]`，变换为`V[k],V[k+1],...,V[n-1],V[0],V[1],...,V[k-1]`，称为*循环左移*。相应地，变换为`V[n-k],V[n-k+1],...,V[n-1],V[0],V[1],...,V[n-k-1]`，称为*循环右移*。循环左移和循环右移的实现方法大同小异，这一小节只讨论循环左移，右移的情况是对称的。

循环左移的过程可以表示为`(V[0:k],V[k:n])`$arrow.r$`(V[k:n],V[0:k])`。这个形式非常类似于交换。相信您一定知道最经典的交换函数的实现：
```cpp
template <typename T>
void swap(T& a, T& b) {
    auto tmp { std::move(a) };
    a = std::move(b);
    b = std::move(tmp);
}
```

#h(2em)一个最朴素的想法，就是用类似的辅助空间，暂存`V[0:k]`中的元素，然后通过3次移动来完成循环左移，如 @fig:vec11 所示。

```cpp
void operator()(Vector<T>& V, std::size_t k) override {
    Vector<T> W(k);
    std::copy(V.begin(), V.begin() + k, W.begin());
    std::move(V.begin() + k, V.end(), V.begin());
    std::move(W.begin(), W.end(), V.end() - k);
}
```

#h(2em)这一算法的时间复杂度是$Theta(k)+Theta(n-k)+Theta(k)=Theta(n+k)$。考虑到$k=O(n)$，这一时间复杂度也可以简化为$Theta(n)$。空间复杂度则为$Theta(k)$。

下面的目标则是将空间复杂度降到$O(1)$。为了保持时间复杂度仍然为$Theta(n)$不变，需要尽可能_一步到位_地移动元素。当我们让`V[i+k]`移动到`V[i]`的位置上时，需要暂存`V[i]`到辅助空间去。下一步，如果继续将`V[i+k+1]`移动到`V[i+1]`的位置，那么需要的辅助空间就会增大。为了防止辅助空间增大，则需要考虑“不用暂存”的元素：也就是已经被移动的`V[i+k]`。下一步将`V[i+2k]`移动到`V[i+k]`的位置，这就是不需要新的辅助空间的。

#figure(
    image("images/vec11.svg", width: 80%),
    caption: "仿照交换元素，用三次移动实现循环左移",
) <fig:vec11>

因此可以得到一个算法：将`V[i+k]`移动到`V[i]`，再将`V[i+2k]`移动到`V[i+k]`，以此类推。最后一次赋值，将辅助空间里的`V[i]`拿出来赋给`V[i-k]`即可。这样实现了一个“轮转交换”的功能。
// 由于向量中的元素是有限的，您可以证明，存在`j`，使得$(i+j k) \% n = i$，也就是经过$j$次移动后回到了$V[i]$。

需要注意的是，这样一轮并不一定能经过`V`中所有的元素。比如在`n=6, k=2, i=0`时，只轮转交换了`V[0],V[2],V[4]`这3个元素，而对另外3个元素则没有移动。我们可能需要进行多趟“轮转交换”，各趟共享同一个辅助空间。@fig:vec12 展示了`n=12,k=3`的轮转交换例子。

#figure(
    image("images/vec12.svg", width: 80%),
    caption: "用轮转交换进行循环位移",
) <fig:vec12>

下面证明：对任意的秩`r`，都存在唯一的$0 <= i < d$和$0 <= j < n/d$，使得`r = (i+jk) % n`。其中`d = gcd(n,k)`是`n`和`k`的最大公因数。

因为$r$和数对$(i,j)$的取值范围都是$n$元集，所以只要证明$(i,j)$到$r$是单射，就蕴含了它同时是满射。因此，只需要证明对于不等的$(i_1,j_1)$和$(i_2,j_2)$，都成立$(i_1+j_1k) \%n eq.not (i_2+j_2k)\%n$。

假设存在整数$q$，使得$(i_1-i_2)+(j_1-j_2)k+q n=0$。由于$(j_1-j_2)k+q n$必定是$d$的倍数，而$|i_1-i_2|<d$，所以只能有$i_1=i_2$。设$k=k_1d,n=n_1d$，那么$(j_1-j_2)k_1+q n_1=0$。因为$(k_1,n_1)=1$，所以$j_1-j_2$必定是$n_1$的倍数，但$|j_1-j_2|<n/d=n_1$，所以只能有$j_1=j_2$。这和$(i_1,j_1)$与$(i_2,j_2)$不等矛盾，故由反证法得到单射成立。

因此遍历顺序为：依次从`0,1,...,d-1`出发，以`k`为步长遍历$n/d$次回到起点。

```cpp
void operator()(Vector<T>& V, std::size_t k) override {
    auto d { gcd(V.size(), k) };
    for (auto i { 0uz }; i < d; ++i) {
        auto tmp { std::move(V[i]) };
        auto cur { i }, next { (cur + k) % V.size() };
        while (next != i) {
            V[cur] = std::move(V[next]);
            cur = next;
            next = (cur + k) % V.size();
        }
        V[cur] = std::move(tmp);
    }
}
```

#h(2em)这一算法的时间复杂度是$Theta(d) dot Theta(n/d) = Theta(n)$，空间复杂度降低到了$O(1)$。

在教材@邓俊辉2013数据结构 上介绍了另一种解法：三次*反转*（reverse）。基于反转的原理是，为了实现`(V[0:k],V[k:n])`$arrow.r$ `(V[k:n],V[0:k])`，本质上是需要让后半段移动到前半段，前半段移动到后半段。而反转恰好能实现这个功能，且不需要移动算法那样的额外空间。在第一次反转后，`V[0:n]`变为了`W[n:0]`（这里`W`表示反转后的向量`V`），我们可以将它拆分为`(W[n:k],W[k:0])`，然后再将这两段分别反转即可，如 @fig:vec13 所示。

```cpp
void operator()(Vector<T>& V, std::size_t k) override {
    std::reverse(V.begin(), V.end());
    std::reverse(V.begin(), V.begin() + k);
    std::reverse(V.begin() + k, V.end());
}
```

#figure(
    image("images/vec13.svg", width: 50%),
    caption: "用三次反转进行循环位移",
) <fig:vec13>

三次反转的时间复杂度同样是$Theta(n)$，而空间复杂度是$O(1)$。从理论上分析：三次移动的做法，写入内存的次数为`k+(n-k)+k=n+k`；轮转交换的做法，写入次数为`n`（每个元素都一步到位地写入了目标位置）；三次反转的做法，写入次数为`n+(n-k)+k=2n`。显然，轮转交换的写入次数最少，三次移动次之，三次反转最多。但实际上，由于三次移动中开辟$Theta(k)$的空间（并自动做零初始化）和释放本身需要时间，所以它在`k`比较大的时候反而会慢于三次反转。#linebreak()
#graybox[【C++学习】#linebreak()#h(2em)
在C++中可以使用`std::rotate`直接进行循环位移。在这个例子中，编译器的标准库实现和优化策略会对三种方法的性能产生显著影响，您可以使用MSVC进行编译，然后和本书中展示的GCC编译结果进行对比。
]

#figure(table(
    columns: 5,
    [`k`], [`std::rotate`], [三次移动], [轮转交换], [三次反转],
    [1], [68], [85], [388], [130],
    [10000], [88], [89], [605], [114],
    [$10007$（素数）], [81], [81],  [1129], [117],
    $5 times 10^7$, [61], [221], [208], [116],
    $9 times 10^7$, [101], [321], [211], [117]
),
    caption: "向量按值删除算法的时间",
) <tab:vec5>

如 @tab:vec5 所示，虽然理论上轮转交换的写入次数是`n`，但它的时间消耗上下浮动很大（并不像三次移动那样随`k`变大而变大），并且一些情况下消耗的时间会远多于三次移动和三次反转。这是算法的_局部性_导致的。三次反转虽然不总是速度最快的算法，但对于各种情况的`k`性能相当稳定。 // TODO:《矩阵》章，具体机制参见《组成原理》。

== 本章习题

在 @vec:capacity-size 中：#linebreak()
+ #medium 假设我们预先分配了大小为$N$的内存空间给`m_data`，并且不允许重新申请内存空间，那么此时向量的扩容上限为$N$。在这种场景下应该如何设计`reserve`函数进行扩容？
+ #hard 假设我们预先分配了大小为$N$的内存空间作为内存池，在扩容的时候，`m_data`不从操作系统申请新的内存，而是从内存池中未被占用的空间里申请新的内存。开始时，`m_data`指向内存池的起始地址，`m_capacity`为0，随着向量的规模增大不断扩容。在这个场景下讨论等比扩容$q$的选择问题。
+ #medium 自动扩容可以在装填因子达到1并且还需要增加规模时进行；自动缩容则应该在装填因子小于某个阈值$theta$的情况下进行。采用分摊分析的方法，分析等差缩容和等比缩容的时间复杂度。
+ #hard 同时采用公比为$q_1$的等比扩容和公比为$q_2$的等比缩容，分析这种策略的_最坏_分摊时间复杂度。
+ #hard 一个$k$位的二进制计数器@cormen2022introduction 使用`bit`的向量存储，计数器的初值为0。假设每次操作是将计数器加1（当计数器的值达到$2^k$时，计数器的值被重置为0），在这种情况下，计数器的分摊复杂度是多少？假设每次操作是将计数器加1或减1（当计数器的值达到$2^k$或小于0时，计数器的值被重置为0或$2^k$），在这种情况下，计数器的分摊复杂度又是多少？

#h(2em)在 @vec:insert 中：#linebreak()
+ #easy 为什么插入中采用了`std::move_backward`，而删除中采用了`std::move`？
+ #medium 在示例代码中使用了 @vec:capacity-size:expand-strategy 中定义的扩容策略来实施扩容，那么扩容的时候，需要先申请一片内存，将原来的数据复制到新的内存中；随后再将`V[r:n]`向后移动1个单位。因此在示例代码中，`V[r:n]`被移动了两次。请设计一个插入时的自动扩容方法，使得`V[r:n]`只需要移动一次。

#h(2em) 在 @vec:average-analysis 中：#linebreak()
+ #medium 如果分摊分析中的一系列操作都是无后效的，那么分摊复杂度是否和平均复杂度相同？
+ #easy 对一个等比（$q=2$）扩容的空向量连续插入一系列元素，每次被插入的元素插入在每个位置是等可能的，在此条件下计算插入的最坏、最好和平均分摊时间复杂度。
+ #hard 对一个等比（$q=2$）扩容的空向量连续插入一系列元素，每次被插入的元素必定插入在上一次插入位置的前面或后面（前面的概率为$p$，后面的概率为$q$，$p+q=1$），在此条件下计算插入的平均分摊时间复杂度。

#h(2em) 在 @vec:find 中：#linebreak()
+ #easy 如果向量中存在多个等于`e`的元素，那么示例代码中会返回第一个等于`e`的元素的位置。请设计一个查找方法，返回最后一个等于`e`的元素的位置。并分析该算法的时间复杂度。
+ #easy 网络流量通常满足Zipf分布。当使用向量存储路由表时，如果将热门站点放在路由表的前面，就可以加速路由查找。假设`e`在向量中的位置服从Zipf分布，即$f(x) = x^(-alpha ) / ( sum_(k=1)^n k^(-alpha) )$，讨论查找成功时的平均时间复杂度。

// #h(2em) 在 @vec:insert-continuous 中：#linebreak()
// + #medium 在向量末尾插入元素（不考虑扩容）的时间复杂度为$O(1)$。因为等比扩容的分摊时间复杂度也为$O(1)$，所以使用连续插入的方法观察扩容时间，其中会有相当一部分的时间用在了插入元素而不是扩容上。设计实验在$n=10^8$的量级下，评估不同公比的等比扩容的性能差异。

#h(2em) 在 @vec:concat 中：#linebreak()
+ #easy 分析一次性移动做法下合并向量的时间复杂度，并在`r`取等可能的情况下计算平均时间复杂度。
+ #medium 向量合并的逆操作是向量拆分。如果希望将向量中的某一段`V[r:r+m]`拆分出来成为一个新的向量，其他部分保持不变，设计一个拆分的算法，并分析时间复杂度。如果不需要拆分，只是将`V[r:r+m]`从原向量中删除，那么时间复杂度会有什么变化？
+ #medium 如果内存没有被预先分配，则向量合并还会涉及一次扩容。类似 @vec:insert 的第2题，设计一个合并时的自动扩容方法，使得`V[r:n]`只需要移动一次。

#h(2em) 在 @vec:remove-value 中：#linebreak()
+ #easy 将删除每一个等于`e`的元素改为删除第一个（或最后一个）等于`e`的元素。设计算法并分析时间复杂度。
+ #medium 设计一个算法，在向量中查找一个连续的子向量，使得子向量的和最大。分析算法的时间复杂度。
+ #medium 设计一个算法，在向量中查找一个连续的子向量，使得子向量的和是大于某个给定值`k`的所有和中最小的那个。分析算法的时间复杂度。
+ #medium 设计一个算法，在向量中查找一个数对`(a, b)`，使得`a+b`是小于某个给定值`k`的所有和中最大的那个。分析算法的时间复杂度。
+ #medium 设计一个算法，在向量中查找一个数对`(a, b, c)`，使得`a+b+c`等于给定的值。分析算法的时间复杂度。
+ #medium 设计一个算法，在向量中查找一个数对`(a, b)`，使得`b-a`的绝对值最大。分析算法的时间复杂度。
+ #medium 设计一个算法，在向量中统计满足`b-a`的绝对值等于给定值的数对`(a, b)`的个数。分析算法的时间复杂度。

#h(2em) 在 @vec:merge-sort 中：#linebreak()
+ #easy 在归并排序中，如果划分的时候，左半部分不取$n/2$而是取`kn`（其中`0<k<1`），时间复杂度会变成多少？如果左半部分取作$max (n/2, C)$，其中`C`是一个给定的常数，那么时间复杂度又会变成多少？
+ #medium 设计一个算法，对`k`个有序序列（`k>2`）进行归并。
+ #medium 设计一个算法，对两个严格有序的序列进行归并，但两个序列中的重复项只保留一个。
+ #hard 使用`merge`函数合并两个有序序列，其长度分别为`m`和`n`。该合并的结果共有$C_(m+n)^m$种（相当于`m`个0和`n`个1组成的0-1串数量），假设发生每种合并结果是等可能的，那么合并过程中`cmp`平均被调用多少次？ // TODO: 就地归并，放在《块状表》
+ #hard 记最坏情况下合并两个长度为`m`和`n`的有序序列的最小比较次数是$m at n$，证明：当$|m-n|<=1$时，$m at n = m+n-1$。
+ #medium 证明：$(m_1+m_2) at n <= m_1 at n + m_2 at n$。
+ #hard 证明：$m at n <= m at floor( n/2 ) +m$。

#h(2em) 在 @vec:merge-sort-bottom-up 中：#linebreak()
+ #easy 证明自下而上的归并排序中，每一趟的时间复杂度为$Theta(n)$。
+ #medium 使用递归的技术，将正文中的自下而上的归并排序的实现改为自上而下，使其调用`merge`的次序和正文中的自下而上的实现相同。
// + #hard 自然的归并排序（natural merge sort）是一种自下而上的归并排序，它的每一趟首先将向量分成若干个有序段（而不是像正文那样分为固定长度的段），然后将两两归并。写出该算法并分析时间复杂度。 // TODO: 放在 《排序》，Tim排序那里

#h(2em) 在 @vec:merge-sort-prior 中：#linebreak()
+ #medium 证明确定$L$的示例代码正确性，并分析此法实现归并排序的时间复杂度。
+ #hard 用本节讨论的两种策略改写自下而上的归并排序，使得在基本有序的情况下时间复杂度更低。分析改进后的时间复杂度。
// + #medium 当向量基本有序，只有开头的长度为$L$的一小段前缀是乱序的时候，讨论自然的归并排序的时间复杂度。

#h(2em) 在 @vec:in-place-merge-sort 中：#linebreak()
+ #easy 为什么经典归并中，在归并的最后只需要将前半部分多余的元素移动回原向量，而在原地归并中，后半部分多余的元素也需要处理？
+ #easy 证明原地归并排序的时间复杂度为$Theta(n log n)$。
+ #easy 举例说明：在原地归并的过程中，用作辅助空间的区域`L`中的元素有可能被直接交换到左侧，也有可能先被交换到右侧若干次再被交换到左侧。
+ #medium 为什么说普通的归并排序是稳定的？

#h(2em) 在 @vec:binary-search 中：#linebreak()
+ #easy 证明折半查找的时间复杂度为$Theta(log n)$。
+ #easy 假设待查找元素在向量中的位置服从均匀分布，分析顺序查找和折半查找的平均时间复杂度。
+ #medium 假设待查找元素在向量中的位置服从几何分布，分析顺序查找和折半查找的平均时间复杂度。什么情况下，顺序查找的性能会超过折半查找？
+ #hard 假设一个$m times n$的矩阵的每行和每列都是严格有序的。给定一个元素`x`，设计算法在矩阵中查找`x`，并分析时间复杂度。
+ #veryhard 假设一个$m times n$的矩阵的每行和每列都是严格有序的。给定一个元素`x`，证明：为了判定`x`是否存在于该矩阵中，最坏情况下最少需要$m at n$（定义见 @vec:merge-sort 的第5题）次比较。


// TODO: 所有关于随机数的算法全部放在《散列》一章中讨论。
// #h(2em) 在 @vec:shuffle 中：#linebreak()

// + #easy 设随机数生成器的生成范围为`[0,m)`。证明：所有$X_(n+1) = f(X_n)$类型的随机数生成器都存在周期`T`（即，当`n`充分大时恒有$X_(n+T) = X_n$）。

// + #hard 在C语言标准库中的`rand()`函数通常采用线性同余生成器（LCG）而不是梅森旋转算法。以下是线性同余生成器的一个简单实现：
//     ```cpp
//     std::size_t rand() {
//         static std::size_t s { 0 };
//         s = (s * 1103515245 + 12345) & 0x7fffffff;
//         return s;
//     }
//     ```
//     证明上述算法可以生成所有$2^31$以内的非负整数。
// + #medium 一般的LCG算法可以表示为`s = (s * a + b) % m`。证明：如果`b = 0`，那么LCG必然不能生成`m`以内所有的非负整数。此类问题的一般结论见@knuth1997artv2。
// // + #hard 证明：如果`a`和`m`互素，则`s`的初始值必然会出现在周期中。
// // + #veryhard 证明当满足以下条件时，LCG可以生成`m`以内所有的非负整数：（1）`b`和`m`互素；（2）`m`的任意素因子`p`都满足`a % p = 1`；（3）如果`m`是4的倍数，那么`a % 4 = 1`。
// + #veryhard 常用的随机数生成器通常更重视生成随机数的速度而不是安全性，我们可以很轻易地通过观察连续的几个随机数去推测参数，从而预测之后的随机数。在LCG算法中，如果已知`m`但未知`a, b`，至少需要观察连续的多少个`s`值，就可以唯一确定`a, b`？

#h(2em)在 @vec:unique 中：#linebreak()
+ #easy 什么情况下，“删除-擦除”法会导致$Theta(n^2)$的时间复杂度？
+ #easy 分析采用 @vec:remove-value 中的其他按值删除算法替换“删除-擦除”法的条件下，唯一化的时间复杂度。
+ #medium 比较“删除-擦除”法和逐个删除法的性能差异。
+ #hard 利用信息熵方法证明，证明唯一化的时间复杂度有下界$Omega(n log n)$。

// #pagebreak()

// (S+1)n = F(n+1) -> 这个放到《二叉树》里

// #h(2em)在 @vec:rotate 中，#linebreak()
// + #easy 证明轮转交换

== 本章小结

向量（顺序表）是程序设计中最经常使用的数据结构，因此本章具有较大的容量。向量的顺序结构是简单的，向量的循秩访问特性是自然的，无论是学习还是考试，这一章的重点都在算法设计上。本章通过较多的实验展示了一些算法设计的典型技巧。其中，对于排序和查找两个重点内容，我们还将在后续的算法章节再次讨论。

本章的主要学习目标如下：
+ 您有了对算法设计中的不必要工作的优化意识。
+ 您学会了对顺序表的整块进行操作，而不是对单个元素进行操作。
+ 您学会了使用快慢指针进行探测和更新。
+ 您学会了利用分支的不对称性改变划分方式以优化分治算法。
+ 您了解到可以从信息的观点分析时间复杂度问题。
+ 您了解到排序预处理可以为后续问题的解决提供帮助。
+ 您了解了分析分摊复杂度和平均复杂度的意义。
+ 您深化了对递归-迭代关系的认识，掌握了消除尾递归的方法。

#h(2em)向量本身是很简单的数据结构，记忆它的插入、删除、查找和各种变形，也是相对简单的；重要的地方在于希望您能理解并掌握书中这些典型算法的设计思路、优化思路。希望本书的内容能为您解决基于向量的算法设计问题提供帮助。